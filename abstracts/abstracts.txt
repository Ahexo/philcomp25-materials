Page 1A defense of computer-assisted proofs from the mathematical
practice
Authors:
Dana Andrea García Carrillo, Instituto de Investigaciones Filosóficas
Topic: Philosophy of Science and Mathematics in the Era of Computing Machinery
Keywords: Computer-assisted proofs, automated proof., mathematical understanding, philosophy of
mathematical practice
This talk is about the epistemic value of computer-assisted proofs (CAPs), based on the case
of the automated resolution of Robbins’ problem using the EQP system in 1996. A major
objection to this type of proof is that, as they are leibnizian style proofs (Hacking, 2014) and
operante on a formal level, they do not offer mathematical understanding, since their
inferences, although valid, are inaccessible to direct human inspection.
However, the above criticism loses force when approached from a philosophy of
mathematical practice. According to Avigad (2008), understanding a proof does not involve
going through each of its inferential steps, but rather deploying active epistemic
competence: identifying assumptions, reformulating it, extending it, and evaluating it
critically. For his part, Robinson (2000) argues that the explanatory component of a proof
does not come from its formal structure, but from its ability to cognitively transform the
agent, which he calls cognitive performativity.
From this perspective, CAPs can facilitate understanding when they are integrated into
practices where the user can manipulate, analyze, and apply them in broader contexts. The
case of Robbins’ Algebras illustrates how an automated proof, although opaque with respect
to its internal processes, allows formal dependencies to be constructed and results to be
extended, thus manifesting a form of situated understanding. That is why, far from
hindering understanding, CAPs can be seen as active epistemic instruments for
contemporary practice.
Page 2A game for feasible classical propositional reasoning
Authors:
Alejandro Solares-Rojas, Instituto de Ciencias de la Computación, UBA-CONICET
Ondrej Majer, Institute of Philosophy, The Czech Academy of Sciences
Favio Miranda-Perea, Departamento de Matemáticas, Facultad de Ciencias, UNAM
Topic: Theory of Computation, Formal Languages, Logic, and Foundational Mathematics
Keywords: classical propositional logic, depth-bounded reasoning, game semantics, tractability
We put forward a game semantics for the "weak depth-bounded" tractable approximations
to classical propositional logic introduced by Marcello D'Agostino and co-authors. Our
semantics models an agent's computational or cognitive resource consumption in terms of
the expense of questions out of a budget within a win-lose two-player game of perfect
information. Increasingly better players can intuitively be associated with increasingly
stronger approximations. Furthermore, the game can be adapted so as to approximate non-
classical reasoning.
Page 3A holistic approach to ethical evaluation and transparency at
every stage of artificial intelligence development
Authors:
Melisa Lizbeth Saavedra Gutiérrez, Instituto de Investigaciones Filosóficas, UNAM.
Topic: Ethical Issues in and of Computing
Keywords: Epistemology, artificial intelligence, data bias., ethical framework
The separation of ethical and epistemic considerations in the evaluation of artificial
intelligence has led to significant shortcomings in addressing issues of bias, opacity, and
accountability on the part of the companies that fund them.
The essay addressed the vision proposed by Russo in Connecting ethics and epistemology of
AI. In the first section, it reconstructed his argument to show that his proposal opens up the
landscape for new questions about the possibility of integrating an ethical framework into
epistemic evaluation methods in AI development. In addition, the discussion addresses the
extent to which a robust ethical and epistemic evaluation framework can contribute to trust
in artificial intelligence. Based on a review of the vision of Russo et al. (2023), in the second
section I show that their proposal does not indicate a clear ethical framework that can be
used when designing, programming, and implementing AI. Along with this, I addressed as
an example the problem of implementing a prediction system without a clear epistemic-
ethical framework in the US criminal justice system. In the third section, I discuss the
possibility of complementing the authors' proposal with a deontological ethical framework
for artificial intelligence. Finally, in section four, I review the limitations of the complete
epistemological ethical framework, using the example proposed above, together with a
conclusion to the debate.
The impact of the use of AI in government agencies for justice is a worrying situation
because: AI needs an epistemological and ethical framework that guarantees transparency
from its modelling to its use. Therefore, this paper addresses the limitations of Russo et al.'s
(2023)  proposed  ethical-epistemological  framework  for  its  implementation.  Beyond
suggesting a complete ethical framework, I focus on highlighting the problem of having an
incomplete framework for specific cases: the use of AI in the US criminal code.
 
Page 4A Modular and Composable Approach to Algebraic Effects in
Systems Programming
Authors:
Victor Borja, Apache Software Foundation
Topic: Philosophy of Computing and Computer Science
Keywords: Rust, algebraic effects, effect handlers, modularity, safety, systems programming
Algebraic effects and handlers are a powerful abstraction for structuring programs by
separating  effectful  computations  from  their  interpretation.  In  systems  programming
languages, such as Rust, their adoption has been limited by challenges in safety, efficiency,
and modularity. This work presents a library-based approach to algebraic effects in Rust,
leveraging ownership, borrowing, and traits to provide a safe, efficient, and composable
framework for effectful programming. Effects are modeled as traits ("abilities"), and
handlers  as  trait  implementations,  enabling  modular  composition.  The  core  monad
encapsulates  effectful  computations,  with  combinators  and  procedural  macros  for
ergonomic  sequencing.  Our  implementation  is  entirely  library-based,  incurs  minimal
runtime overhead, and requires no compiler changes. We demonstrate applications in state
management, error handling, and dependency injection. This approach shows that algebraic
effects can be practical and powerful for systems programming, opening new possibilities
for modular and safe code in Rust and similar languages.
Page 5A Pragmatic Defence of the Physical Church-Turing Thesis
Authors:
J. F. W. Smiles, University of Bristol
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Agent-relative computability, Computational realism, Finite agents, Hypercomputation,
Implementation, Individuation of computation, Operationalism, Physical Church–Turing Thesis, Symbolic
control, Thermodynamic constraints
I defend a pragmatic reinterpretation of the Physical Church–Turing Thesis (PCTT), arguing
that computability should be understood not as an intrinsic property of physical systems, but
as a constraint on what finite agents can operationally stabilise and exploit. Existing
approaches to physical computation typically treat any state-transition system as a potential
computer, provided a structure-preserving mapping can be drawn from physical states to
abstract symbols. But this confuses the act of embedding a computational model within a
physical model with the presence of computational power in the physical system itself. A
computational description is only adequate if it does more than redescribe the dynamics.
Indeed, the salient point is that without a finite agent to construct, maintain, or verify the
mapping, the system computes nothing—in any operationally meaningful sense. 
By foregrounding the agent-relative nature of computation, I argue that the real boundary
captured by the PCTT is practical, not metaphysical. Computation arises only when physical
mechanisms are diachronically stabilised as symbol-like states under conditions that allow
finite agents to control and reuse them. This requires energy, memory, noise suppression,
and  representational  conventions—resources  that  hypercomputational  proposals
systematically ignore. Such models posit precision and stability that no agent can implement
or verify, and thus fall outside the domain of physical computability in this operational
sense.
This operationalist view reframes the PCTT as a structural consequence of finitude: the
asymptotic boundary of what can be constructed and sustained by embedded agents in a
noisy  world.  Rather  than  undermining  the  PCTT,  this  relativisation  reinforces  its
significance  by  linking  it  to  real-world  thermodynamic  and  epistemic  constraints.
Computation, like thermodynamic work, is defined not by formal possibilities but by
practical controllability.
Page 6A Trustless Decentralized AI Schema Based On Zero-
Knowledge Proofs
Authors:
Carlos Efrain Quintero Narvaez, School of Engineering and Sciences, Tecnológico de Monterrey
Nezij Mendoza Castellanos, Facultad de Ciencias, UNAM
Topic: Frontier Artificial Intelligence, Neurocomputation, and Computational Linguistics
Keywords: AI Governance, Blockchain, Cryptography, Decentralized AI, Ethereum ZKSync, Philosophy of
Technology, Trustless Systems, Zero-Knowledge Proofs
In this paper, we propose a theoretical schema that illustrates how an architecture similar
to ledger protocols such as Ethereum, Bitcoin, and ZKSync could be used to build a trustless
decentralized AI network using Zero-Knowledge Proofs (ZKPs). We begin by critically
assessing the dominance of centralized AI platforms like OpenAI and highlighting the
epistemic and governance challenges they pose. In response, we investigate how the
principles underlying trustless cryptocurrency systems align with the philosophical and
practical demands of decentralized AI. We then examine decentralized compute models like
the Ethereum Virtual Machine (EVM), noting their redundancy-related inefficiencies. To
address these, we explore advancements such as ZKSync, which leverages ZKPs to enable
scalable and verifiable decentralized computation. Building on this foundation, we propose a
schema for a decentralized AI framework that incorporates zkLLM, a recently introduced
ZKP method for verifying Large Language Model (LLM) computations. We conclude by
analyzing the incentive structure of such a network and the possible emergence of an
unstoppable AI.
Page 7Abstractions and representations in software construction
Authors:
Israel Sandoval Grajeda, Instituto de Investiagaciones en Matemáticas Aplicadas y en Sistemas UNAM
Topic: Philosophy of Computing and Computer Science
Keywords: abstractions in software, computational thinking, problem solving process
Nowadays,there are a lot of software development methodologies and technologies that help
guide the programmer in the development process, allowing the related tasks to be done
easier. However, solving a problem and representing a possible solution as code in a
programming language is a mental effort - not a mechanical action- that involves, both,
knowledge of how the problem could be solved, and a method or technique to convert the
solution to algorithms to be programmed and run in a computing device.
The latter  action can be seen as a language translation, the main difference  being that the
target language of the solution  has more rigorous rules than the source language.  
Thus, an abstraction from the original solution written in natural language to an algorithm
representation implies a simplification because the building of an algorithm has previously
established rules, similar to how a programmer develops code from an algorithm, since a
programming language is more restrictive than algorithms.
Nonetheless, if the solution is very complex, building an algorithmic representation does not
follow  a  straightforward  path.  For  this  reason,  my  approach  is  to  include  other
representations between the problem solution in natural language and the algorithmic
representation to make the process easier with successive representations at different levels
of abstraction.
This topic opens a discussion about how we can define levels of abstraction and how this
could be useful to define the concept of complexity  applied to the process of problem
solving.
Page 8AI and Society
Authors:
Luis Alberto Dacomba Méndez, UNAM
Topic: Computational Philosophy
Keywords: AI, cognitive processes, creativity
In a constantly changing world, the integration of tecnology and society has become
increasingly evident and inevitable. The unprecedented generation of data has driven the
implementation of artificial intelligence in different areas of our lives, from industry and
research to education and more. As a result, AI is becoming more than just a tool; it's
becoming an increasingly essential component.
However, as this integration progresses, different questions and concerns arise, such as:
"Will AI eliminate job opportunities?" and "Does the use of generative AI affect our cognitive
processes and creativity?"
This paper seeks to explore and analyze these issues, as well as their ethical and existential
implications for a society increasingly immersed in technology.
Page 9AI as Acceleration of Ontological Reduction: From
Technofeudalism to Dystopian Transhumanism
Authors:
Sergio Luna, UNAM
Topic: Gender, Politics, and Society in Computing
Keywords: Accelerationism, Algorithmic Governance, Artificial Intelligence (AI), Computational Capitalism,
Digital Reductionism, Epistemology and Computation, Ontological Assumptions of the Human,
Posthumanism, Technofeudalism, Transhumanism
In recent years, artificial intelligence has been heralded as a revolutionary step in human
progress. However, this paper argues the opposite: AI has become the ultimate tool for
accelerating the most alienating and oppressive structures already embedded within techno
feudalist society. Using ideas from the philosophy of computer science, critical theory, and
the legacy of accelerationism, this work proposes that AI functions not as a liberatory
transhumanist project, but as a dystopian amplifier of inherited ontological assumptions
about "human nature." These assumptions, which are rooted in colonial epistemologies,
mechanistic reductionism, and an aristocratic tradition of academia, are embedded in the
very architectures of computational systems.
The rise of AI within this context demonstrates how new technologies reinforce old
hierarchies. Unlike earlier technological overhypes (i.e., quantum computing, NFTs), AI's
accessibility gives users the illusion of participation in the technocracy, masking the deeper
logic of extraction and control. This paper explores how algorithmic reasoning inherits a
view  of  the  human  as  predictable,  quantifiable,  and  network-traceable  as  seen  in
surveillance capitalism patents. Through this lens, AI becomes not a neutral tool, but an
agent of epistemological closure and ontological flattening.
Ultimately, here, it is argued that the philosophy of computer science must critically address
how computation, in particular AI, reshapes what it means to know, to govern, and to be
human. This paper offers a framework for understanding AI as a philosophical event: not
because of its intelligence, but because of how it operationalizes and accelerates the
dominant ideologies of our time.
Page 10Algorithmic Aesthetics: How Artificial Intelligence Shapes
Normative Taste in Digital Environments
Authors:
Natzue Mendoza Jaimes, -
Topic: Aesthetic Issues in and of Computing
Keywords: Algorithmic Aesthetics, Artificial Intelligence, Artificial Subjectivity, Digital Visual Culture,
Platform Ethics, Recommendation Systems
This  paper  explores  how  artificial  intelligence,  through  algorithmic  recommendation
systems on social media platforms, not only organizes but actively shapes aesthetic norms in
contemporary digital environments. Far from being neutral filters, these algorithms curate
our exposure to visual content, influencing taste, desire, and cultural legitimacy. By
privileging specific styles, bodies, and visual trends, and rendering others invisible,
algorithmic systems establish normative standards that govern what is seen, valued, and
emulated.
Drawing on interdisciplinary perspectives from aesthetics, media theory, and digital ethics,
the paper argues that social media users are increasingly configured as aesthetic subjects
through continuous engagement with algorithmically curated feeds. This process results in
the homogenization of desire, the illusion of aesthetic diversity, and the gradual erosion of
pluralism in visual culture.
The work calls for a critical reevaluation of aesthetic autonomy in computational contexts
and proposes the need for alternative curatorial models that resist the logic of optimization.
Ultimately, the paper aims to initiate a philosophical conversation about the ethical
responsibilities of digital platforms in shaping cultural sensibilities and the role of AI in the
construction of normative taste.
Page 11Algorithmic Forgetting
Authors:
Jesus Armando Tapia Gallegos, Self
Topic: Ethical Issues in and of Computing
Keywords: AI and Human Rights, Algorithmic Forgetting, Ethical AI, Machine Unlearning, Privacy-
Preserving AI, Right to Be Forgotten (RTBF), Value-Sensitive Design
The  ongoing  evolution  of  artificial  intelligence  (AI)  presents  both  unprecedented
opportunities  and  profound  ethical  challenges  for  organizations  navigating  digital
transformation.  This  study  examines  the  intersection  of  transformational  and  agile
leadership with responsible AI implementation in hybrid and remote work environments.
Drawing from empirical insights and current the
Artificial intelligence systems have become central to how memory and identity are stored,
processed,  and  retrieved  in  the  digital  age.  Yet  their  persistent  nature  challenges
fundamental rights, particularly the Right to Be Forgotten (RTBF). While legally recognized
in several jurisdictions, including the European Union, the implementation of RTBF within
AI infrastructures remains unresolved both technically and ethically.
This proposal introduces a conceptual and technical framework for algorithmic forgetting,
offering  design  pathways  for  building  AI  systems  that  can  respect  RTBF  without
compromising model integrity or transparency. It explores recent advances in machine
unlearning,  targeted  obfuscation,  and  regulatory  modeling,  while  engaging  deeper
philosophical questions: What does it mean for a machine to “forget”? Who decides what
should be forgotten, and under what ethical logic?
Rather than treat forgetting as an anomaly to be patched, this approach repositions it as a
normative value, to be designed into AI from the ground up. This argues that effective RTBF
implementation  must  blend  legal  compliance,  ethical  foresight,  and  computational
innovation—requiring  active  collaboration  across  disciplines.
The contribution is both timely and original, speaking directly to the ethical design of
intelligent systems in a world where remembering everything is no longer neutral, but
potentially harmful.
Page 12Algorithmic justice and emotions in socio-digital networks
Authors:
María Virginia Bon Pereira, Universidad de Monterrey
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Algorithmic justice; Emotion; Intersubjetivity; Communication; Socio-digital; Networks; Ethic.
Socio-digital communication technologies and platforms are currently agents of construction
and intersubjective, social, cultural, economic and political change.
However, socio-digital technologies and the use of increasingly refined algorithms as
opaque  contribute  to  the  transformation  of  interpersonal  and  intersubjective  human
relations, as well as to the perception of the world and the subject itself (Vinck, 2018;
Antolinos, 2019; Balladares, 2023; García & Bailey, 2021). In this sense, the New Digital
Pact (UNESCO, 2024), promotes a human-centered interaction, highlighting the axiological
as the basis of an ethics regulating all digital communication.
The present research aims to consider socio-digital networks and artificial intelligences
linked to communications, from a personalistic perspective and under a philosophical
ethical approach. It is considered that technologies should be at the service of human
beings and should always contribute to social improvement and well-being. The human
being must administer and regulate technologies under certain ethical parameters such as
the principle of justice, beneficence and not malefficiency; the principle of autonomy, the
principle  of  human  supervision  or  governance  of  autonomy  and  the  principle  of
explainability  and  transparency.
Page 13An antagonist for AI: presenting the need to cultivate an
alternative way of encoding meaning
Authors:
Annabel Castro Meagher, Universidad de Monterrey
Topic: Gender, Politics, and Society in Computing
Keywords: IA antagonist, Touch language, classification stories, communication ecosystem, words' engine
The objective of this paper is to present the need to cultivate an alternative way of encoding
meaning other than through AI driven systems. The multiplication of digital devices and
applications (aiming to maximize user interactions that produce data from which behavioral
patterns can be discovered) has become more than a disturbing continuous noise in our
living. The way we live has been transformed into a way of living that can be measured and
sensed continuously by digital systems. We are tricked or forced into continuously feeding
these systems with the large amounts of data they need, modifying our behavior to transpire
our data in a convenient format for the system. Intelligence is a term that isolated us from
the rest of the animal kingdom. Artificial intelligence empowered devices are also isolating
us: they render obsolete the gaining of knowledge through how our body experiences the
world.
What if we close our eyes and ears to shut down the digital devices that surround us,
liberate our hands from them and communicate with one person at a time, in a language
based on touching each other’s body? Let our lives be modified by such a language. 
Through this paper I study the body’s relation to meaning, words’ engine potential,
intelligence’s relation to words, intelligence’s antagonists, the power of classification stories
and touch as a language meant for the living. At last I conclude by speculating how
cultivating a language based on touching bodies can bring balance into our communication
ecosystem. 
Page 14Apuntes para una Filosofía Política de la Inteligencia
Artificial
Authors:
Wulfrano Arturo Luna Ramírez, Universidad Autónoma Metropolitana Cuajimalpa
Topic: Philosophy of Computing and Computer Science
Keywords: Decolonialidad, Filosofía Política, Filosofía de la ciencia y la tecnología, Inteligencia Artificial
Hoy  en  día  la  Inteligencia  Artificial  (IA)  ha  cobrado  gran  relevancia,  al  menos
mediáticamente, entre otras cosas gracias al empuje que las herramientas generativas le ha
dado. Resurgen también preocupaciones por las distintas problemáticas implicadas por su
desarrollo y uso. La reflexión filosófico-política ocasionado por la IA, abordan cuestiones
como: poder, gobierno, libertad, manipulación, explotación, esclavitud, justicia, racismo,
sexismo. El análisis de la IA respecto de ciertos temas de la Filosofía Política (FP)
comprenden: igualdad, democracia, totalitarismo, participación, exclusión, vigilancia y
autodeterminación.  Se  han  tocado  también  discusiones  ambientalistas,  especistas  y
transhumanistas.
Es necesario pensar la IA desde un inventario teórico que considere saberes de la periferia,
la teoría crítica y decolonial, la discusión de los pensadores de nuestra tradición científico-
filosófica, quienes generan una reflexión que adelanta muchos de los temas discutidos hoy
en día en las metrópolis tecnológicas en torno a las implicaciones filosóficas, ético-morales,
y económico políticas de la IA.
La FP incide en el cuerpo teórico de la IA, para discutir más ampliamente su base
epistemológica: considerar otras formas de racionalidad además de la racionalidad moderna
(lógica-matemática); los conceptos de agencia e intencionalidad, enfocados en el individuo y
no en la colectividad; revalorar la Hipótesis Situada; ampliar el ámbito de aplicación de la IA
hacia los problemas sociales; y otros conceptos que en Filosofía de la Computación y de la
IA, atañen cuestiones ideológico-políticas (ej. la singularidad o la distinción algoritmo-
programa).
Ampliar la visión de la FP de la IA, requiere cuestionar las racionalidades práctica (sus
fines)  y  teórica  (sus  fundamentos)  con  planteamientos  de  nuestros  pensadores
iberoamericanos.  Tal  es  el  cometido  de  esta  propuesta.
Page 15Artificial Authorship. Generative Artificial Intelligence and
Authorship in the Age of Large Language Models
Authors:
Antonio Díez, Instituto de Investigaciones Bibliotecológicas y de la Información (IBII)
Topic: Philosophy of Computing and Computer Science
Keywords: Authorship, Creativity, Generative Artificial Intelligence, Intellectual Property, Large Language
Models, Originality
Abstract:
This dissertation examines the notion of authorship in light of the growing popularity of
Large Language Models (LLMs). Drawing on the philosophy of technology, it aims to
critically reflect on the key features that define what it means to be an author. In particular,
it considers the idea that authors enter into contractual agreements and manage intellectual
property rights. Moreover, in the social imagination, an author is understood as an
individual  endowed  with  creativity  and  talent,  capable  of  establishing  an  intimate
connection with readers.
However, legal frameworks evolve historically, and originality is inherently subjective.
LLMs, for their part, are able to imitate human syntactic structures, enabling them to
produce coherent texts. This raises an important question: can LLMs be considered
authors? There are already documented cases in which individuals have written and
published books with the assistance of LLMs. In such contexts, should the prompt engineer
be regarded as the author, and the LLM as a co-author? What, ultimately, distinguishes a
human author from an artificial one?
Guided by the philosophy of technology, this presentation seeks to explore the human
boundaries of authorship and to reflect on the underlying motivations of writing—why we
write, and for whom.
Page 16Artificial intelligence and the academic world
Authors:
Julio César Martínez Sánchez, Escuela Nacional de Ciencias Forenses
Topic: Ethical Issues in and of Computing
Keywords: Artificial intelligence, ChatGPT, ethics in the digital age
When we include texts or data generated with Artificial Intelligence in an academic work,
who holds authorship over those elements? Who is responsible for the validity of the ideas?
And more importantly, can we trust AI’s contributions to scholarly work? In this talk, we
reflect on the role of language models, such as ChatGPT, in the production of academic
content and highlight both their usefulness and their risks. Using a specific example — a
prompt requesting a paragraph with citations and data—we examine how AI can produce
writing  that  appears  coherent  and  well-supported,  even  when  it  includes  fictitious
references or select authors based on popularity rather than relevance. The key issue is that
current regulation is virtually nonexistent. For this reason, we propose starting a dialogue
within the academic community to establish clear guidelines that promote the responsible
use of AI. We believe this conversation is urgent: as these systems continue to evolve, it will
become increasingly difficult to distinguish between content written by humans and that
generated by algorithms.
Page 17Atmospheric Computing: A Philosophical Approach to Digital
Noise as a Technical, Symbolic, and Environmental Interface
Authors:
M. C Clara Mendoza Sánchez, Benemérita Universidad Autónoma de Puebla
Dr. José Víctor Rosendo Tamariz Flores, Benemérita Universidad Autónoma de Puebla
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Critical ecology., Digital noise, Digital subjectivity, Philosophy of computing, Technological
atmospheres
 
Atmospheric Computing: A Philosophical Approach to Digital Noise as a Technical,
Symbolic, and Environmental Interface
In contemporary urban and digital environments, noise is no longer reducible to an acoustic
phenomenon: it has become synonymous with data overload, stimulus saturation, and
algorithmic interference. This paper offers a philosophical investigation into digital noise,
conceptualized as a liminal interface between computational systems and embodied
experience, approached through technical, epistemological, and symbolic dimensions.
From a technical perspective, noise manifests as signal distortion (Shannon, 1948),
informational entropy, and non-pertinent or erroneous data that undermines the efficacy of
machine learning and predictive modeling (Zhu & Wu, 2004; Bishop, 2006). Emerging
paradigms such as noise-based logic (Kish, 2010) and robust analog computing (Wang et al.,
2025; Chung et al.,  2025) suggest that noise may also serve as a computational vector,
challenging conventional signal/noise dichotomies and enabling novel algorithmic
architectures.
Philosophically and culturally, recent scholarship such as Portella Castro (2022) and
Malaspina (2018) has rearticulated noise as ground : a generative, irreducible margin that
resists normalization and encodes indeterminacy. Drawing on these insights, this paper
introduces the concept of atmospheric computing as a philosophical category that frames
computation not merely as symbolic logic but as a distributed, affective infrastructure that
configures contemporary digital and urban atmospheres (Sterne, 2012; Morton, 2013).
This theoretical framework further engages with the Frankfurt School’s Critical Theory,
particularly the critique of instrumental reason (Horkheimer & Adorno, 1947) and
technological alienation (Marcuse, 1964), and connects with Berry’s concept of the
algorithmic condition (2025), wherein digital systems mediate perception and normalize
subjectivation.
Ultimately, rethinking digital noise through a philosophical lens not only questions the limits
of computational representation, but also opens up space for critical and resistant practices that recognize noise as a creative, ontological, and political potential.
Page 19Beyond the Algorithm: Distributed Epistemic Opacity and the
Role of Empirical Adequacy in xAI 2.0
Authors:
Juan Francisco Ortiz Moreno, Universidad Nacional Autónoma de México
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Epistemic opacity, Trustworthy AI, explainable artificial intelligence (xAI), model
interpretability, stakeholder's desiderata
This  paper  challenges  the  common  assumption  that  epistemic  opacity  in  artificial
intelligence systems is confined to algorithmic complexity. Instead, it argues that opacity is
distributed across the full lifecycle of machine learning systems, encompassing data
generation, model training, and deployment decisions, many of which remain inaccessible or
invisible to users, developers, and regulators alike. Drawing on the concept of essential
epistemic opacity  (Humphreys 2009) and recent elaborations (Durán & Formanek, 2018;
Greif, 2022), the paper identifies three core loci of opacity: data provenance and labeling,
design  of  training  sets,  and  specification  of  modeling  objectives.  These  dimensions
contribute to structural and procedural opacity that cannot be resolved through algorithmic
inspection alone.
In response to these challenges, the paper proposes a shift in explainable AI (xAI) from the
ideal of transparency  to that of empirical adequacy . While transparency demands full access
to  internal  mechanisms,  empirical  adequacy—borrowed  from  philosophy  of
science—evaluates explanation methods based on their ability to track and predict model
behavior across relevant input-output relationships. This reorientation allows for a more
pragmatic and epistemically grounded standard in xAI 2.0, particularly when dealing with
complex deep learning systems.
The paper concludes by advocating for explanation frameworks that prioritize operational
accountability and contextual reliability over full interpretability. By acknowledging the
distributed nature of epistemic opacity and embracing empirical adequacy, xAI research can
better align with the needs of real-world users, domain experts, and policy-makers in
fostering trustworthy AI systems.
Page 20Building personal identity in the digital age: Between the
physical and the virtual
Authors:
Rinnette Riande González, Master’s Program in Philosophy of Science, UNAM
Topic: Ethical Issues in and of Computing
Keywords: affordances, digital identity, personal identity, scaffoldings, social media
Building personal identity in the digital age: Between the physical and the virtual
Nowadays, it is practically impossible to separate virtual activities from our daily lives. From
paying for services to interpersonal communication, our digital presence often seems to be
on par with or even more important than our physical presence. This raises a key question:
what happens to personal identity in this context? Although it is complex to consider virtual
activities as something completely different from daily experiences, there is a tendency to
believe that a person can divide their experiences and develop distinct identities: one in the
real world and another in the virtual environment.
This phenomenon is reflected in people who create alter egos in forums, social networks,
and video games. How is personal identity constructed in the digital realm? Can we
continue to consider them the same person, despite experiencing different realities and
acting in different ways? This presentation seeks to answer the question through a more
flexible perspective of personal identity that includes the affordances (possibilities for
action) that digital technology offers for personal development.
I will explore how technological possibilities allow us not only to experience the world in
different ways, but also to perceive ourselves and act in novel ways. In addition, I will
analyze the role that emerging technologies such as artificial intelligence, algorithms, and
the collection of personal data, including biometric data, play in the creation and expression
of a digital identity.
Page 21BURIDANIAN INDECISION IN DECISION-MAKING AGENTS
Authors:
Juan Armando Ramírez García, UNAM
Topic: Philosophy of Computing and Computer Science
Keywords: Indecision, epistemic state, paradox, thought experiments
It is not far-fetched to assert that one of the most interesting thought experiments in the
history of ideas is Buridan's donkey. It is not only a thought experiment, but a paradox with
implications in various branches of speculative and practical philosophy, which can be
extended to decision-making, computational systems, epistemic logic, automated problem-
solving, and artificial intelligence as a decision-making agent.
Buridanian indecision questions the very existence of rational agents, whatever their nature.
This indecision is not due to a lack of information, but is inherent to the nature of the
decision itself. If the agent decides to make a decision, it must do so with the available
information, since more data do not change the agent's epistemic state.
In Buridanian indecision, we are not facing equal options, but rather indifferent options,
that is, it does not necessarily imply that the options are equal in all aspects, but rather that
the agent does not have a clear preference between them. This paper reviews the genesis of
Buridanian indecision in the history of ideas, analyzes the relevance of adopting this notion
in the field of cybernetics and computation, explores its scope, and inquires about the
prospects of formalizing such a notion.
Page 22Closure and the Internal Logic of Life: A Structural
Convergence Across Biology, Logic, and Computation
Authors:
Andrés Ortiz-Muñoz, N/A
Topic: Theory of Computation, Formal Languages, Logic, and Foundational Mathematics
Keywords: autocatalysis, autopoiesis, category theory, closure, computation, deduction theorem, lambda
calculus, logic, self-organization, type theory
Closure is a foundational concept appearing across biology, logic, and computation, often
associated  with  autonomy,  self-reference,  and  the  capacity  for  self-maintenance.  In
theoretical  biology,  closure  arises  in  organizational  schemes  such  as  autopoiesis ,
autocatalytic sets , and closure to efficient causation , each emphasizing the idea that a
system can produce and sustain the very processes that define it. These notions are central
to origins of life research, where closure is often taken as a necessary condition for life.
In logic, deduction theorems  express closure under implication: if a conclusion 'B' follows
from a premise 'A', then the implication 'A → B' follows from the axioms of pure logic. In
computation, structural analogs appear in the lambda calculus , where abstraction  enables
the formation of function terms from derivations, and in type theory , where the introduction
rule for function types  allows functions to be constructed from proofs that assume an input.
Closed categories  in category theory abstract these formal mechanisms, unifying logical and
computational closure.
This talk explores how these diverse forms of closure may reflect a deeper structural
principle. Using an abstract model of chemistry introduced by Fontana, I argue that closure
in biological systems can be formally related to closure in logic and computation. This
connection suggests a shared internal logic, a structural condition for autonomy and
inference, that spans from living systems to formal systems, and may offer a biologically
inspired lens through which to understand computation and deduction.
Page 23Cognitive warfare: origin and development of a concept
Authors:
Laura Danniela Cárdenas Carrillo, Political and Social Sciences Faculty, National Autonomous University
of Mexico
Topic: Gender, Politics, and Society in Computing
Keywords: Cognitive warfare, Cyber warfare, Information warfare, Psychological warfare, Soft power
operations, Surveillance Capitalism
The creation of the concept of cognitive warfare  is relatively recent and, however, its
development is as rich and complex as the implications of its implementation in the recent
past, as well as in the near future, are transcendental.
The aim of this abstract is to track the origin and development of the concept of cognitive
warfare in order to highlight the context from which it was born and developed, as well as
its rapid evolution, both semantic and operational, based on the crossroads between
psychological and influence operations (soft power) on the one hand, and cyberwarfare
operations on the other, and which approach requires a multidisciplinary perspective that
allows us to account for the tremendous military capabilities that it opens up and that
actually have not been sufficiently studied, as well as the ethical dilemmas and the probable
civil rights restrictions that would be arise from the development of military programs with
clear surveillance and censorship overtones that have not yet regulated by the law, and
whose consequences on civil life must be weighed.
Page 24Communication strategies in COIL virtual migration projects
Authors:
Magda Lillalí Rendón García, FCPyS, UNAM
Topic: Gender, Politics, and Society in Computing
Keywords: , Artificial Intelligence, COIL, Communication Networks, Digital Literacy, Global Citizenship
Communication strategies in COIL virtual migration projects are essential for achieving
participant interaction in the digital space, especially considering the variety of elements
involved in these multicultural activities, which are carried out remotely from different
geographical  areas  in  the  same  virtual  space,  with  synchronous  and  asynchronous
communication.
Therefore, different learning styles, different work areas, and diverse worldviews contribute
to the comprehensive development of digital citizens.
 
Díaz said “The internet user then takes on an active role, able to create and disseminate
information. At the same time, virtual platforms also provide the opportunity to interact,
comment, and express opinions on shared information, using memes as social criticism”.
(2022)
Also, the use of resources and tools is based on the knowledge (knowing, doing, being, and
being),  learning  styles,  emotional  management,  and  expectations  of  instructors  and
participants.
It’s important considerate synchronous communication spaces such as Zoom or Meet, the
use of collaborative work resources such as Google Slides, or storage resources such as
Google Drive or Dropbox, are part of the knowledge acquisition spaces required for
communication strategies to learn how to apply them, transform, and modify coexistence
strategies in daily life, both personally, professionally, and academically.
 
Finally, with this presentation, I will present a quick workbook on what we need to know to
communicate always, and especially in virtual spaces. Including concepts such as Global
Citizenship,  Digital  Literacy,  Artificial  Intelligence,  COIL,  Communication  Networks,
because  with  the  strategies  make  a  better  way  of  interacting  with  others.
Page 25Computational Discovery of Therapeutic Targets: In Silico
Modeling of TATA-Binding Protein in Chagas Disease
Authors:
Carlos Gaona, Universidad Veracruzana
Topic: Biocomputation, Biomathematics, and Artificial Life
Keywords: Drug repositioning, TBP, Trypanosoma cruzi, cheminformatics, computational tools
Parasitic diseases like Chagas remain a global health challenge, exacerbated by drug
resistance and limited therapeutic options. This study illustrates how computational tools
can drive new modes of knowledge production in biomedical research. We applied an in
silico modeling pipeline—combining bioinformatics, cheminformatics, homology modeling,
molecular docking, and molecular dynamics simulations—to systematically evaluate over
11,000 FDA-approved compounds against the TATA-Binding Protein (TBP) of Trypanosoma
cruzi .
Our computational screening pipeline led to the identification of two compounds, DB00890
and DB07635, as promising candidates for targeting the TATA-Binding Protein (TBP) of
Trypanosoma cruzi.  These molecules demonstrated favorable interaction profiles in silico,
suggesting their potential as inhibitors of this key transcription factor. To complement and
validate the computational predictions, we conducted a series of in vitro biological assays.
Notably, DB00890 exhibited moderate trypanocidal activity, effectively reducing parasite
viability across different T. cruzi strains. Moreover, DB00890 displayed a moderate degree
of selectivity, affecting the parasite while maintaining acceptable cytotoxicity levels toward
host macrophage cells (J774.2). This integrated approach—combining digital modeling with
experimental validation—highlights the potential of computational methods not only to
accelerate drug discovery but also to inform the design of targeted, selective therapies for
complex parasitic diseases.
Beyond its biomedical contributions, this work exemplifies how digital computation is
reshaping epistemic practices in drug discovery—redefining what counts as evidence, how
molecular interactions are modeled, and how therapeutic hypotheses are generated. We
reflect on the philosophical implications of in silico modeling in science, particularly in
addressing complex biological systems.
Page 26Computational Psychiatry, Interoception, and Endorhythmic
Scaffolding
Authors:
Somogy Varga, Aarhus Universtiy
Topic: Frontier Artificial Intelligence, Neurocomputation, and Computational Linguistics
Keywords: Computational psychiatry, cognition, computational models, interoception
Computational  psychiatry  increasingly  models  interoception  (i.e.,  the  perception  and
regulation of internal bodily signals) as central to understanding mental disorders. Two
challenges arise. First, how can the pervasive influence of interoception on cognition be
understood? Standard predictive coding accounts often treat interoceptive signals as static
inputs, yet growing evidence shows that cognition is rhythmically modulated by endogenous
bodily cycles such as respiration and heartbeat. Here, the framework of endorhythmic
scaffolding clarifies how these rhythms provide temporal structures that entrain neural
activity and scaffold cognitive processes. Second, how should we interpret the striking
heterogeneity of interoceptive findings across disorders and individuals? Rather than
methodological noise, this may reflect stable interoceptive subtypes requiring specific
computational models. Together, scaffolding and heterogeneity suggest that computational
psychiatry must expand toward richer accounts of how interoception shapes cognition and
varies across populations.
Page 27Contrasting conceptual frameworks to assess neural
networks: Between habitus and collective intentionalities
Authors:
Javier Toscano, Alice Salomon Hochschule, Berlin, Germany
Topic: Frontier Artificial Intelligence, Neurocomputation, and Computational Linguistics
Keywords: Neural networks, collective intentionalities, critical algorithm studies, habits
This presentation offers a comparative analysis of two conceptual frameworks— machine
habitus  and collective intentionality —to critically assess neural networks (NNs) and large
language models (LLMs) as socio-technical systems. Drawing on Massimo Airoldi’s Machine
Habitus  (2022), the study first explores how neural networks, through training, acquire
dispositions analogous to human habitus (inspired by Bourdieu), internalizing patterns that
reflect the social and cultural biases of their data environments. This framework enables a
critical understanding of how social inequalities and historical biases are reproduced in
machine behavior.
In contrast, the notion of collective intentionality , as developed from Michael Tomasello and
reinterpreted by Javier Toscano (2022a, 2022b), foregrounds the collective and institutional
dimensions of algorithmic functioning. Rather than viewing AI systems as isolated agents,
this perspective sees them as emergent phenomena within broader socio-computational
infrastructures. It posits that NNs express forms of pre-reflective collective cognition,
embedded in the intentions and practices of the human institutions that develop, deploy,
and regulate them.
The presentation examines the philosophical and historical lineages of both concepts (e.g.,
Barandiaran & Di Paolo 2014; Brejdak 2021), analyzing their epistemological foundations
and applicability to AI. Through a systematic comparison, it evaluates the strengths and
limitations of each framework in explaining machine learning processes, decision-making
dynamics, and normative alignment.
Crucially, the research considers the ethical implications of these approaches: How do
"machine habits" perpetuate structural injustice? What responsibilities emerge if AI systems
are seen as bearers of collective intentionalities? By bridging theoretical inquiry and AI
critique, the study contributes to debates in AI ethics and epistemology, offering tools for
understanding neural networks not merely as technical systems, but as entangled actors
within human social and institutional life.
Page 28Conversatorio TIES - Revista de Tecnología e Innovaciónen
Educación Superior.
Authors:
Arturo Muñiz Colunga, DITE, DGTIC - UNAM
Topic: Gender, Politics, and Society in Computing
Keywords: Educación, humanidades digitales, tecnología
Este conversatorio tiene como propósito presentar y dialogar en torno al más reciente
número  de  TIES,  Revista  de  Tecnología  e  Innovación  en  Educación  Superior,  una
publicación semestral de acceso abierto editada por la Universidad Nacional Autónoma de
México (UNAM), a través de la Dirección General de Cómputo y de Tecnologías de
Información y Comunicación (DGTIC). La revista se ha consolidado como un espacio
especializado para la divulgación científica sobre innovación en cómputo y tecnologías
digitales aplicadas a las Instituciones de Educación Superior (IES), ofreciendo una mirada
crítica, propositiva y actualizada del papel de la tecnología en los procesos de enseñanza,
investigación y gestión académica.
El número 12 de TIES reúne cinco artículos que muestran la diversidad, profundidad y
relevancia de las discusiones actuales en torno a la tecnología y la educación superior.
Ernesto  Priani  ofrece  una  reflexión  sobre  los  fundamentos  epistemológicos  de  las
humanidades digitales, abriendo una discusión sobre su estatuto científico. Iván Meza y
colaboradores  presentan  un  estudio  experimental  sobre  la  inducción  de  rasgos  de
personalidad en un Modelo Masivo de Lenguaje (MML). Vicente Torres examina los sesgos
algorítmicos en imágenes forenses generadas mediante inteligencia artificial, destacando
implicaciones éticas y técnicas. Suyin Ortega y Ricardo Tavira evalúan el desempeño de
herramientas tipo Retrieval-Augmented Generation (RAG) para la búsqueda de información
académica, mientras que Ángel Barrios y su equipo exploran el valor didáctico de
GUI_srsRAN_5G, una interfaz gráfica de código abierto para redes 5G privadas.
Este conversatorio busca propiciar el intercambio de ideas entre autores, lectores y
especialistas interesados en el impacto y las posibilidades de la tecnología en el ámbito
universitario, promoviendo una lectura crítica y colaborativa de los temas abordados.
Page 29Del flujo de electrones a la experiencia: Análisis de la
materialidad digital, sensorialidad e interacción persona-
ordenador. Aportes desde la experiencia háptico-auditiva.
Authors:
Martín A Lozano-Nevárez, Centro de Investigación en Ciencias Cognitivas
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: cognición corporeizada, corporalidad, dicotomía virtual/real, interacción digital, materialidad
digital
Este trabajo propone una crítica profunda a la dicotomía tradicional entre lo virtual y lo real
en la interacción humano-computadora. A través del estudio de experiencias digitales
háptico-auditivas de personas con discapacidad visual, se demuestra que toda experiencia
computacional está arraigada en una materialidad expandida, donde cuerpo, tecnología y
cultura coexisten de forma indisoluble. Se incorporan enfoques desde el enactivismo y la
antropología de los sentidos para argumentar que lo digital no es una abstracción
inmaterial, sino una realidad corporal y simbólica. La materialidad digital opera en dos
niveles: uno biológico-físico, que transforma señales sensoriales en datos digitales; y otro
sociocultural, que dota de significado a dichas señales. Entre sus aportes filosóficos
destacan una nueva ontología digital centrada en el cuerpo, una epistemología basada en la
sensorialidad, y una crítica al oculocentrismo dominante. Los entornos mediados a través de
las  interacciones  hápticas-auditivas  se  presentas  así  como  un  espacio  revelador  de
experiencias digitales alternativas, ricas en significados sensoriales e implicaciones éticas y
tecnológicas.
Page 30Densification and Emotion-Driven Network Evolution in Open
Social Systems
Authors:
Shaunette T. Ferguson, Barnard College, Columbia University
Topic: Gender, Politics, and Society in Computing
Keywords: affective states, complex netwworks, densification scaling
Human interaction has shifted from direct, face-to-face communication to large-scale, digital
discourse where individuals engage with both content and each other. Platforms like
YouTube provide an environment for dynamic social interactions, where discussions emerge
around video content and evolve over time. These engagement networks operate as open
systems, expanding and contracting as users freely enter and exit discussions. Many
empirical networks exhibit a relationship between total edges () and active users () that is
consistent with a densification scaling property, where  increases superlinearly with . In
temporal social networks, this dynamical property between  and  is influenced by (i)
fluctuations in population size (), (ii) changes in the probability of node connections (), or
(iii) both. Given a fixed connection probability and a growing population, conventional
superlinear scaling emerges. Conversely, for a fixed population size and increasing
connection probability,  exhibits an accelerating growth pattern, where  increases with  at a
rate higher than conventional superlinear scaling.
In this study, we investigate how network structures and emotional expressions shape the
evolution of YouTube discussion networks. We specifically examine variations in network
structure via estimates of network population  and activity level (), and their relationship
with global affective states over time. Up to now, studies exploring structural change in
networks have primarily relied on global network metrics ( and ), treating affective states as
mere byproducts of interaction. With the availability of more detailed interaction data that
includes users’ emotional expressions, we can now investigate the role that emotions play in
influencing densification.
 
Our results show consistency in the scaling pattern across datasets across diverse forms of
discourse, from diplomatic discussions to highly polarized debates. This suggest that a
universal structural feature; namely, the open-nature  of online systems is likely responsible
for the conventional scaling. We also find that emotions are more than byproducts.
Page 31Desintegración del futuro: ideología de la desesperanza,
ciberespacio y capitalismo
Authors:
Andrés Jiménez Lizárraga, Facultad de Ciencias (UNAM)
Topic: Gender, Politics, and Society in Computing
Keywords: Capitalismo, Ciberespacio, Filosofía de la tecnología, Hauntología, Ideología, Marxismo,
Melancolía de izquierda, Nostalgia política, Redes sociales
Esta  investigación  constituye  un  primer  acercamiento  al  estudio  de  las  principales
manifestaciones en el ciberespacio de un fenómeno sociocultural: la idea colectiva sobre la
presunta imposibilidad de concebir un futuro alternativo al capitalismo. Llamamos a este
fenómeno la «ideología de la desesperanza».
Buscamos exponer algunos mecanismos ideológicos en los que el modelo neoliberal se
apoya para neutralizar las manifestaciones políticas, sociales y culturales alternativas al
estado de cosas actual. A partir de los términos realismo capitalista  y precorporación
(Fisher 2016), explicamos el surgimiento de sentimientos colectivos de desesperanza e
incapacidad de reconfigurar el futuro a causa de las transformaciones geopolíticas de los
años noventa y su papel en la conformación de ideologías y expresiones socioculturales que
rescatan ideas, prácticas, estéticas y productos culturales del siglo XX.
Se toman las nociones de melancolía de izquierda  (Brown 1999), hauntología y superyó
leninista  (Fisher 2018, 2024) para describir la idealización de épocas pasadas específicas y
los  intentos  de  regresar  a  ellas.  Para  entender  las  manifestaciones  socioculturales
nostálgicas dentro del ciberespacio, retomamos tratamientos marxistas del concepto de
ideología (Gramsci 1967, 1971; Eagleton 1997; Žižek en Fiennes 2012), mismos que nos
permiten explicar la relación entre el neoliberalismo y el desarrollo de la «ideología de la
desesperanza».
Asimismo, para develar los desafíos ideológicos y epistémicos de este fenómeno en el
ciberespacio, empleamos los análisis sobre los problemas derivados de la aplicación de
modelos algorítmicos en las actividades cotidianas (O’Neil 2016), el fenómeno de las
burbujas de filtros (Pariser 2017) y las implicaciones sociopolíticas del uso de redes sociales
(Van Dijck 2013).
Finalmente, proponemos que la difusión de la ideología de la desesperanza en el
ciberespacio actúa como un recurso ideológico capaz de producir la inmovilización política
de  los  individuos,  como  también  la  desmovilización  de  las  actividades  izquierdistas
contemporáneas.
Page 32Desiring Machines and Digital Biopolitics: A Critique of
Predictive Algorithms
Authors:
Eric Rodríguez Ochoa, The Ludwig Wittgenstein Project
Topic: Ethical Issues in and of Computing
Keywords: Desiring machines, algorithmic resistance, digital biopolitics, digital ethics, individual
autonomy, predictive algorithms
This proposal examines how predictive algorithms operate as “desiring machines” within a
framework  of  digital  biopolitics,  extending  Deleuzian  theory  into  contemporary
computational  practices.  By  analyzing  the  anticipatory  mechanisms  embedded  in
recommendation engines and behavioral profiling systems, the study reveals how algorithms
pre‑emptively shape user preferences and subjectivities—often without overt consent or
awareness. Building on Michel Foucault’s notion of biopower, the research interprets
large‑scale data governance as a form of population control, where algorithmic prediction
becomes a tool for managing life processes in real time.
The project unfolds in three phases. First, it deconstructs the technical architectures of
predictive algorithms to demonstrate their alignment with Deleuze and Guattari’s concept of
desiring‑production. Second, it situates these processes within a biopolitical regime,
highlighting ethical concerns around autonomy, surveillance, and consent. Third, it explores
modes  of  resistance,  from  open‑source  alternatives  to  critical  design  interventions,
proposing pathways to re‑envision human‑machine co‑evolution. The research contributes to
debates in technopolitics, algorithmic ethics, and philosophy of computation by offering a
nuanced critique of how digital desire and governance intersect.
This study will appeal to scholars in AI ethics, computational philosophy, and cyberculture
studies,  providing  theoretical  insights  and  practical  recommendations  for  rethinking
predictive systems in an era of algorithmic biopolitics.
Page 33Discussing authorship through AIs: Legal ethical implications
of the adoption of intelligent technology in the Copyright
sector
Authors:
Jesus Manuel Niebla Zatarain, Universidad Autonoma de Sinaloa - Facultad de Derecho Mazatlan
Virginia Berenice Niebla Zatarain, Tecnologico Nacional de Mexico - ITES Los Cabos
Gonzalo Armienta Hernandez, Facultad de Derecho Culiacan - Universidad Autonoma de Sinaloa
Topic: Ethical Issues in and of Computing
Keywords: Legal artificial intelligence - Authorship - Copyright law
AI is profoundly impacting areas once deemed incompatible with this technology, sparking
ethical and legal debates concerning the copyright and legal status of AI-generated works.
This proposal's core aim is twofold: first, to examine the effectiveness of authorship in the AI
era. Second, to analyze how authorship can be extended to scenarios where AI plays a
substantial role in the creative process. The intention isn't to remove the human element
from copyright but to foster a collaborative approach where AI enhances human creativity.
Traditional copyright law, exemplified by frameworks in the EU, US, and UK, generally
mandates a human element for legal protection. For instance, the EU's Artificial Intelligence
Act requires users to maintain documentation related to AI training models. Similarly, the
US, like the UK's Copyright, Patents and Designs Act of 1988, emphasizes human presence
in creation for protection, though this is often assessed case-by-case.
The research will employ both inductive and deductive methodologies. The former will
analyze current legal positions on AI authorship in the creative sector, while the latter will
evaluate their effectiveness and propose improvements for defining authorship in AI-related
contexts.
This work seeks to establish a clear position on the role of authorship in the creative sector
and its compatibility with the advantages of intelligent technology in the digital age.
References
Charles, D. R. (2025). AI as artist: agency and the moral rights of creative works. AI and
Ethics , 1-12.
Komuves, D., Niebla, J., Schafer, B., & Diver, L. (2015). Monkeying around with copyright:
animals, AIS and authorship in law. CREATe Working Paper , (1), 11.
Quintais, J. P. (2025). Generative AI, copyright and the AI Act. Computer Law & Security
Review , 56, 106107.
Schafer, B., Komuves, D., Zatarain, J. M. N., & Diver, L. (2015). A fourth law of robotics?
Copyright and the law and ethics of machine co-production. AI and Law , 23(3), 217-240.
Page 35Edupunk 2.0 redefiniendo la educación en la era del
tecnofeudalismo
Authors:
Pablo Adrián Rivera Juvenal, Universidad Nacional Rosario Castellanos
Topic: Philosophy of Interaction Design
Keywords: Edupunk 2.0, inteligencias artificiales generativas., tecnofeudalismo, territorios digitales
En la presente ponencia se postula la propuesta educativa denominada Edupunk 2.0, la que
es una reinterpretación del modelo Edupunk surgido en el inicio del siglo XXI y cuyo lema
era “hazlo tú mismo”.
Esta propuesta se vio limitada por la complejidad de la programación de aprendizaje para
los agentes educativos que no se vinculaban directamente con el campo de la informática;
por lo que fue perdiendo fuerza a mediados de la primera década del siglo XXI.
Sin embargo, la ponencia retoma la importancia de volver a recuperar la filosofía y el
modelo de Edupunk en el 2025, debido al control y poder que existe cada vez con más
fuerza del terriroiro digital con finos mercantilistas que se adapta al naciente modelo
Tecnofeudalista.
Por otra parte, la barrera existente en programación se debilitó gracias a las inteligencias
artificiales generativas que permiten la generación de código de forma simplificada y
permite la construcción de objetos y espacios digitales que permiten la apropiación del
territorio digital por medio de la construcción de entornos educativos digitales libres.
Page 36El internet como territorio en disputa
Authors:
Leonardo Tomas Ruiz Mora, FFyL - UNAM
Topic: Gender, Politics, and Society in Computing
Keywords: cultura cibernética, desterritorialización, hegemonía cultural, internet, resistencia digital,
semiocapitalismo, tecnocapitalismo, territorialización
Este trabajo propone una lectura crítica del internet como un territorio en disputa, donde se
libra una constante lucha por la producción de subjetividades, sentidos y deseos. Lejos de
ser un espacio neutral, el internet ha sido progresivamente territorializado por las lógicas
del capital, convirtiéndose en una maquinaria semiocapitalista que captura nuestros deseos
y atención en función de la productividad y el consumo, fungiendo como una herramienta
del realismo capitalista.
Este enfoque se aleja del paradigma computacional clásico y se alinea con una filosofía
ampliada del cómputo, centrándose en su dimensión simbólica, cultural y afectiva. Se
pretende analizar cómo más allá de ser sólo un sistema técnico que transmite información,
es una infraestructura sociotécnica que organiza, condiciona e inscribe formas específicas
de racionalidad, control y subjetivación.
Recuperando la noción gramsciana de hegemonía cultural, se analiza el internet como un
espacio de confrontación ideológica en el que narrativas dominantes y subalternas compiten
por el poder cultural. De esta manera, es posible plantear que puede ser desterritorializado.
Su uso reagenciado abre la posibilidad de prácticas subversivas, como el hacktivismo,
movimientos de resistencia digital, entre otros. Sin embargo, no basta con un uso
alternativo o cambios individuales. Se requiere intervenir en su infraestructura y modos de
producción de sentido: redes comunitarias, servidores autogestionados, software libre,
repositorios digitales abiertos, y proyectos de alfabetización digital crítica.
En suma, se apunta a construir una contrahegemonía cultural desde lo digital, capaz de
interrumpir la lógica capitalista y abrir espacios de cooperación, conocimiento compartido,
autonomía  y  resistencia.  Desterritorializar  el  internet  significa  hackear  sus  formas
establecidas de subjetivación, produciendo líneas de fuga desde dentro del territorio
capturado. Aún en medio de su creciente privatización y captura algorítmica, el internet
sigue siendo un campo donde disputar el poder cultural e imaginar otras formas de vida y
comunidad.
Page 37El nacimiento del algoritmitariado en la trama de la muerte
del capitalismo. A propósito de la sociedad dataísta
Authors:
Abraham González Montaño, Universidad del Desarrollo Empresarial y Pedagógico
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Algoritmitariado, capitalismo, clase algorítmica, dataísmo
La era dataísta ha alterado drásticamente la constitución de las sociedades en el siglo XXI al
modificar y crear para sí innovadores mecanismos de valor y, con ello, nuevos entramados
de  relaciones  sociales  que  ahora  son  mediadas  por  la  producción,  acumulación  y
procesamiento de masivos bancos de datos, con el fin de que sean usados para —por parte
de la clase algorítmica (naciente clase social gobernante)— modular, intervenir, acoplar y
sugestionar algorítmicamente a las personas —ahora convertidos en el algoritmitariado
(nueva clase antagónica)— para que compren, participen, se comporten, relacionen,
interactúen e incluso existan como se calcula de antemano que lo hagan, mientras,
paradójicamente, son explotados al momento en que trabajan de manera gratuita al
producir  cantidades  inmensas  de  información.  Dicho  proceder  relata  una  cosa:  el
nacimiento de la sociedad dataísta y la muerte paulatina de la era capitalista.
Page 38Enhancing Student Learning in the Age of Powerful AI: A
Pedagogical Proposal
Authors:
Qiu Lin, Department of Philosophy/Simon Fraser University
Topic: Gender, Politics, and Society in Computing
Keywords: AI-generated content, Comparative learning, assignment design, in-class discussion, pedagogy
This presentation introduces comparative learning , a pedagogical method where students
critique AI-generated essays using the same rubric applied to their own philosophical
writing. In recognition of the widespread use of various "AI assistants" in students'
academic lives nowadays, this approach invites them to compare an AI-generated product
and its human-generated counterpart. It also exposes AI’s limitations in handling nuance
and complex thought. In this talk, I will address three challenges: tracking AI use, crafting
philosophically rich prompts, and building a course module grading an AI-generated essay
and comparing it to its human-generated counterpart. Rather than endorsing or rejecting AI
outright, this method uses its outputs as a foil to cultivate students' critical skills and
encourage them to think about what it is to have genuine expertise.
Page 39Entre simulación y comprensión: límites epistémicos de los
modelos de lenguaje artificial
Authors:
Francisco Javier Suárez Vargas, Universidad de Guadalajara / Colegio Profesional de la Comunidad
Mexicana de Estudiantes de Filosofía
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Epistemología, Inteligencia artificial, autoridad epistémica, conocimiento
Se afirma en círculos tecnológicos que las máquinas ya no solo realizan cálculos
matemáticos complejos mediante comandos lógicos, sino que pueden comunicarse con los
humanos  gracias  al  lenguaje  precargado  con  entrenamiento  profundo.  A  nivel
epistemológico, Rico Hauswald postula que, de la misma manera en que al ser humano se le
considera una autoridad epistémica (AE), puesto que es capaz de transmitir conocimiento a
otros, también la IA podría ser una autoridad epistémica artificial (AEA) debido a su facultad
de simbolizar y brindar discursos que son, hasta cierto punto, coherentes mediante el
lenguaje textual. 
La cuestión central es revisar la postura de Hauswald, sus argumentos a favor y en contra
de tal tesis, en el marco de la pregunta por la capacidad de la IA de figurar como AEA. En
tal sentido, expondré algunas objeciones por las cuales pienso que la IA no es una autoridad
titular, sino una herramienta auxiliar, puesto que carece de configuraciones perceptivas por
las que se disciernen las nociones conceptivas tanto de entrada como las de salida. Ello hace
que su facultad epistemológica, si es que la tiene, sea incompleta, limitada, o en su caso,
defectuosa, debido a que no tiene en su mecanismo algorítmico la característica igual o
simulada de lo que se considera como la cognición básica perteneciente a animales,
incluídos los humanos. Para explicarlo me apoyaré en la concepción de cognición básica
propuesta por la autora Susan Carey. En un primer apartado se expondrá críticamente la
visión de Hauswald y en un segundo apartado se brindarán las razones, con apoyo del
término de Carey, para negar la titularidad de la IA en procesos de comunicación y de
enseñanza. 
Page 40Epistemic Character of AI as a Complex System
Authors:
Gloria Daniela Martínez Caudillo, Universidad de Guanajuato
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Adaptive system, Artificial intelligence, Complex system, Computation, Emerging Property,
Epistemology, Knowledge, Non-linearity
Is  knowledge  possible  for  artificial  intelligence?  What  would  be  the  necessary
epistemological conditions for this to occur? This paper aims to provide an answer to these
questions through a philosophical analysis. In order to address these inquiries, a hypothesis
will be developed as follows: If artificial intelligence is considered a complex, adaptive, non-
linear system with emergent properties, then it is possible to argue that AI systems are
capable of generating epistemically valid forms of knowledge, even if these do not align with
traditional models of human knowledge theories. This perspective opens up the possibility of
expanding  classical  epistemology—based  on  conscious  subjects—to  include  artificial
systems that produce knowledge. Therefore, the objective is to examine the foundations that
allow AI to be considered a complex system and to assess the role of the concept of
emergence. Additionally, epistemological issues related to justification and explanation will
be revisited, focusing on the scope and limits of knowledge—but directed toward the context
of artificial knowledge and the complexity framework in which AI may be regarded as an
epistemic agent.
Page 41Epistemic opacity vs reliability: some lessons from the binary
offset effect in the SNIFS
Authors:
María Martínez-Ordaz, Instituto de Investigaciones Filosóficas, UNAM
Andrés Vázquez-Quijano, Instituto de Investigaciones Filosóficas, UNAM
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Epistemic opacity, SNIFS, empirical adequacy, reliability
Here, we explore the reliability of measurement processes in contexts governed by high
degrees of ignorance due to heavy technological dependency. In particular, we focus on the
reaction of the cosmologists towards the binary offset effect on the SuperNova Integral
Field Spectrograph (SNIFS) and its consequences on the risk management strategies in the
building of the Dark Energy Spectroscopic Instrument (DESI).
Our main thesis can be summarized as a plea for a broader epistemic shift: rather than
seeking full transparency or eliminability of opacity, scientists must develop new strategies
for justifying trust under persistent epistemic opacity. If we are to make sense of the
reliability of such systems, it is not sufficient to trace the transparency of individual
components; instead, we must understand how local indicators of empirical adequacy and
cross-system coherence come to play a foundational role in legitimizing scientific inference
and practice.
First, we discuss the basics of the scientists’ problematic trust in the products of
epistemically opaque processes. Second, we present in more detail the case of the binary
offset effect in the SNIFS and the corresponding modifications made in DESI.
Third, we explain how the assessment of the empirical adequacy and epistemic reliability in
such contexts would demand a broader epistemic shift. Finally, we present some final
remarks.
Page 42Facial Emotion Recognition Using an Evolutionary
Convolutional Neural Network
Authors:
Edwin Bryan Salas Lopez, UAM
Silvia Beatriz Gonzalez Brambila, UAM
Juan Villegas Cortez, UAM
Diana Jacqueline Chagoya Galvan, UAM
Topic: Frontier Artificial Intelligence, Neurocomputation, and Computational Linguistics
Keywords: Convolutional neural network, Evolutionary algorithms, Facial emotion recognition, Green
computing
Recent advances in machine learning, such as the use of convolutional neural networks,
have driven significant improvements in emotion recognition. However, these achievements
demand high energy use and contribute to a growing carbon footprint. This work draws on
insights from the philosophy of computing, proposing an approach that integrates technical
performance with environmental accountability. A methodology is presented that evaluates
various convolutional network architectures for facial emotion recognition, measuring
accuracy, energy consumption, and CO ₂ emissions during training. Additionally, a coarse-to-
fine model enhanced with evolutionary algorithms is implemented, showing that more
efficient systems can be built without disregarding their broader environmental implications
or the long-term trajectory of technological development.
 
Page 43Gender Bias in Artificial Intelligence Applied to Health: A
Systematic Review
Authors:
Dania Nimbe Lima Sanchez, Departamento de Salud Digital, Facultad de Medicina. UNAM.
Antonio Montero Balderas, Programa de Doctorado en Derecho, Facultad de Estudios Superiores Acatlán,
UNAM
Alejandro Alayola Sansores, Departamento de Salud Digital, Facultad de Medicina. UNAM.
Topic: Gender, Politics, and Society in Computing
Keywords: Artificial Intelligence, Bias., Gender
 
Artificial intelligence has been widely used in the healthcare field, the lack of regulation and
its applications for diagnosis, prevention, and treatment have allowed its use to spread
widely, however, there are risks associated with biases, especially demographic ones such
as gender and populations with a digital divide. This work presents a critical synthesis on
gender biases in AI, with a special focus on the healthcare sector and an ethical reflection
on its impact. A review was carried out on large language models (LLMs) and automated
systems, identifying trends, gaps in the literature, and emerging strategies to mitigate these
biases. The main findings show that structural biases are transferred to algorithms and their
reproduction negatively impacts historically marginalized populations, increasing the gap in
care and behaviors that present significant ethical dilemmas, especially in the healthcare
field.
Page 44Ghost in the shell: El vínculo del hombre con la IA.
Authors:
Kenny Paul Arias García, FES Acatlán
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Ghost in the shell: El vínculo del hombre con la IA ética ontología epistemología
En este breve ensayo se examina el vínculo que el hombre tiene con la inteligencia artificial,
la cual se refiere en sus siglas IA, para entender este vínculo se plantea la IA como una
herramienta, como se le puede entender desde la obra de Schopenhauer, del mismo modo
se plantea su valor como una especie de prótesis que ayuda suplir carencias o ampliar
habilidades debido al texto de Roger Bartra Cerebro y libertad , para fines de dicha revisión
se toma de ejemplo la obra Ghost in the shell  película de anime del año 1995 en donde ya
podemos notar una relación íntima entre el hombre y ciertas herramientas tecnológicas, de
ello también se desprende la importancia de entender cómo usamos lo que está a nuestro
alcance para evitar ingenuidad o ambigüedad, es decir se busca entender el modo que el
humano usa la tecnología como herramienta para su beneficio y como esta IA puede ser
vista.
Page 45Human creativity? The problem of defining when machines
also create.
Authors:
Mario Ulises Maya Martínez, Universidad Anáhuac México
Topic: Aesthetic Issues in and of Computing
Keywords: Creativity, Definition, GenAI, Human, Machines
Creativity has become a topic of interest for researchers in various fields. However, despite
this interest and the increase in empirical studies, aspects such as its definition and
evaluation have been the subject of continuous debate. The “standard definition of
creativity” consider it as: “the ability to generate new (original) ideas that are useful
(appropriate)”. For some experts, this definition is very focused on the production of
something tangible and leaves aside another equally or more important perspective: the
generation of ideas, which involves various cognitive processes.
With the advent of advanced technologies such as the Gen AI, the issue is back on the table
and a series of questions and concerns are generated: Can machines solve problems in the
same way as human beings?, Can the solutions provided by these models be considered
“creative”? In the midst of this debate, the concept of creativity appears at the center. Could
our “humanity” be at stake?
Researchers and laboratories around the world have begun to enter into this debate,
studying the interaction that has been taking place for years between machines and humans
during their creative activity. It is not surprising that we know more and more examples of
“co-creation”. The study of creativity in virtual environments, the use of avatars and
“Divergent thinking bots” has also generated interesting results.
It has become necessary to propose adjustments to the standard definition in order to
differentiate creations made by human beings from those produced by a program. Experts
in the field have published and disseminated “manifestos” on their positions, pointing out
the importance of considering the social, environmental, cultural and legal implications
involved in this phenomenon.
Maybe in the future we will talk about something different, but for now creativity is still part
of what we consider human experience.
 
Page 46HUMAN-AI COORDINATION IN DEMOCRATIC
DELIBERATION PROCESSES
Authors:
Josué H. Bojorges, Centro de Investigación en Ciencias Cognitivas
Topic: Gender, Politics, and Society in Computing
Keywords: Artificial Intelligence (AI), attention, cooperation., democratic deliberation
What are the conditions under which Artificial Intelligence (AI) and humans can coordinate
in democratic deliberation processes? This proposal seeks to describe some parameters of
cooperative interaction between human and artificial agents toward shared democratic
deliberation goals. To do so, we propose attention  as a starting point. As a cognitive
dimension common to both, attention serves as the gateway to describing highly operational
concepts. To achieve this, we aim to: 1) provide a theoretical description of cooperative
interaction between human-artificial agents for joint deliberation goals based on attention,
and 2) identify which human cognitive abilities—beyond attention—are fundamental in this
deliberative interaction. In addition to our theoretical proposal—and to ground our
framework in empirical data—we will present progress in designing a "democratic" chatbot.
This chatbot acts as a mediating agent among human participants to reach democratic
deliberative choices, while evaluating the effects of such coordination on human cognitive
skills.
Page 47Interdisciplinary Complexity and Theoretical Necessity in
Data Science
Authors:
Gerardo Rossel, Universidad de Buenos Aires. FCEyN. Departametno de Computación
Topic: Philosophy of Computing and Computer Science
Keywords: Data Science, Epistemic Field, Epistemology, Interdisciplinarity, Theoretical Frameworks
Data Science has rapidly emerged as a pivotal interdisciplinary field, yet its conceptual
boundaries remain ambiguous. Traditional models—such as Venn diagrams—fail to capture
the true complexity and evolving nature of the discipline. This work proposes a shift in
perspective: instead of viewing Data Science as a mere intersection of skills, it should be
understood as an "epistemic field" —a dynamic, historically situated entity defined by its
theories, applications, practitioners, and societal context.
By adopting this epistemic framework, the article highlights how Data Science integrates
knowledge  from  statistics,  computer  science,  mathematics,  domain  expertise,  social
sciences, and the philosophy of computer science. This convergence enables innovative
solutions to complex problems but also introduces tensions regarding epistemic authority
and disciplinary boundaries. The field’s applied orientation distinguishes it from basic
sciences, emphasizing practical problem-solving across sectors like industry, health, and
finance.
Crucially, the article challenges the notion that Big Data renders theory obsolete. Theory is
not obsolete. Far from it. Theoretical frameworks are indispensable for interpreting data,
guiding inquiry, and transforming raw patterns into meaningful scientific knowledge. The
argument is illustrated through cases such as climate change research, where robust
models are essential to make sense of vast datasets.
The interdisciplinary nature of Data Science also raises ethical and epistemological
challenges, including bias, transparency, and reproducibility. As the field evolves with
advances in AI and deep learning, a flexible yet rigorous conceptual approach is needed to
ensure responsible knowledge production. Ultimately, conceiving Data Science as an
epistemic field fosters a deeper philosophical understanding and supports its responsible
development in the digital age.
Page 48Justifying Homotopical Logic
Authors:
Arnold Grigorian, NRU HSE, Int. Laboratory for Logic, Linguistics and Formal Philosophy
Topic: Computational Philosophy
Keywords: category theory, constructive logic, foundations of mathematics, homotopy type theory,
univalent foundations
This  talk  explores  two  complementary  justifications  for  Univalent  Foundations  and
homotopical logic. The first part addresses the role of internal language  in category theory
as a foundational tool. Categories with sufficient structure—such as topoi—can serve as
models of mathematical universes. Reasoning within these internal languages contrasts with
external, meta-theoretical reasoning, underscoring the importance of distinguishing internal
and external viewpoints in foundational work. Internal reasoning facilitates generalization
and helps avoid non-constructive assumptions typical of classical meta-theory.
The second part adopts a model-theoretic lens to justify the logical status of the Univalence
Axiom. Introduced by Voevodsky, this axiom states that identity of types corresponds to
equivalence—a principle rooted in homotopy theory. In the homotopical interpretation of
Martin-Löf Type Theory, types correspond to spaces, and identity is understood as homotopy
equivalence. The Univalence Axiom thus formalizes the idea that mathematical objects are
invariant under equivalence, providing a precise internalization of this meta-theoretical
principle.
These perspectives converge in the framework of Homotopy Type Theory (HoTT), which
serves as the supposed internal language of ∞-topoi—categorical structures that generalize
and expand set-theoretic models. Beyond its theoretical significance, HoTT has become
central to the formalization of modern mathematics  in interactive proof assistants such
as Coq, Agda, and Lean. These systems leverage  the homotopical and constructive
foundations of HoTT to rigorously formalize complex mathematical theories with machine-
verified proofs.
Page 49Living Algorithms: Coevolving Computation Between
Bacterial Plasmids and Cellular Automata
Authors:
Carles Tardío Pi, Centro de Ciencias Genómicas, UNAM
Topic: Biocomputation, Biomathematics, and Artificial Life
Keywords: bacteria, bioputing, cellular automata, open hardware, plasmids, unconventional computing
We present a heterotic hybrid computing device composed of a coevolving system that
integrates in vivo  bacterial plasmid populations with in silico  elementary cellular automata
(ECA). This biologically-algorithmic coupling enables dynamic updates of the bacterial
growth environment, guided by the interaction between a living, evolving substrate and a
simple  rule-based  system  with  Turing-complete  computational  potential.  The  system
operates autonomously as an unsupervised installation, facilitating a responsive dialog
between a primordial open-ended biological process and a deterministic digital entity.
Physically, the device is actuated by a micro-pipetting robot that performs periodic serial
transfers of bacterial populations and inoculates nutrient media into 96-well plates. A
sensory feedback layer, implemented via a Raspberry Pi–controlled open-source microscope,
measures the optical density of cultures at each time step. These readings inform real-time
updates of the ECA rule via a machine learning layer, creating a closed-loop interaction that
allows the system to adapt across domains.
This architecture yields a non-reducible computational system wherein the cross-domain
entanglement of bacterial and digital states produces emergent dynamics that transcend the
sum of its parts. By propagating dual, coevolving substrates in time, the system enacts a
novel mode of communication between two distinct yet responsive forms of primordial
intelligence—biological and algorithmic—without requiring anthropocentric interpretation
or supervision.
Page 50Machines, Freedom, and Danger
Authors:
Vincent Alexis Peluce, College of Southern Nevada
Topic: Ethical Issues in and of Computing
Keywords: Artificial Intelligence, Compatibilism, Existential Danger, Freedom
Does the possibility of machine freedom pose an existential threat to humanity? In other
words, should we add this to the list of worries related to Artificial Intelligence? In his 1993
Artificial Intelligence , Jack Copeland argues that machines might achieve freedom, but only
the compatibilist sort. I argue that if compatibilist freedom is the only sort of freedom
available to machines, then machine freedom per se  does not pose an existential threat to
humanity. Alternatively, this means that if machine freedom in fact poses an existential
threat, then some freedom beyond that of the compatibilist sort must be available to
machines.
 
Page 51Modeling transitional probability learning and target
detection from a neurocomputational framework
Authors:
Mario E. Fischer Casco, Faculty of Psychology, National Autonomous University of Mexico
Ángel E. Tovar y Romo, Faculty of Psychology, National Autonomous University of Mexico
Topic: Computational and Non-Computational Cognitive Science
Keywords: artificial neural network, neurocomputational model, reaction time, target detection,
transitional probability learning
Statistical  learning,  the  ability  to  identify  recurrent  distributional  patterns  in  the
environment, is implicated in multiple cognitive abilities and complex behaviours, and
numerous  neural  plasticity  mechanisms  are  thought  to  be  involved.  Transitional
probabilities between stimuli are among the main statistical properties that can be
recognized and learned by humans and other species. In a typical Target detection task ,
reaction times are measured and target objects are preceded by either fixed or random
stimuli sequences. During fixed sequences, transitional probabilities are higher and It’s
considered that detection times decrease as a function of learning, thus predictions of target
occurrence are based on preceding stimuli. In this study, we ran several computational
simulations of target detection tasks. The simulations were implemented in artificial neural
networks including three key components: Hebbian learning, residual activation and
Gaussian noise. The model operation has two stages: the statistical learning and response to
target objects ( i.e., target detection). In the learning phase the model strengthens or
weakens the connection weights through a Hebbian learning algorithm that resembles long-
term  neural  plasticity  mechanisms.  In  the  target  detection  phase,  a  response  unit
accumulates activation that comes from both the target unit and from other units through
the Hebbian weights, until a threshold is reached. The model accounts for the main results
obtained in the simulated empirical studies and provides evidence for the cognitive and
neural mechanisms that could be implicated in the task. Furthermore, exploring the
variation of model parameters could provide predictions and explanations about the
performance in the task of populations with learning or memory disabilities such as
participants with autism or Down syndrome. 
Page 52Natural computing: a dialogue between Luis Pineda, David
Marr y Andy Clark
Authors:
Juan Sebastián Novoa Toledo, Universidad Nacional de Colombia
Topic: Philosophy of Computing and Computer Science
Keywords: Natural computing, mode of computing, predictive processing, representation
In his recent work, Dr. Luis Alberto Pineda explains that computing is a mixed process
involving  an  objective  component—namely,  the  mechanical  transformation  of
representations—and a subjective component, which consists in the interpretation of those
representations. In artificial computing, the transformation process is carried out by a
machine, but the interpretation is done by a human being. In contrast, in natural computing
(if it exists at all), both aspects must be performed by the same entity—let’s say, the brain.
However, it is not clear that the brain performs traditionally understood algorithmic
processes, nor is it clear that symbolic representations are involved in its functioning.
Therefore, it remains an open possibility that there is no mode of computing associated with
the brain, and, consequently, that the mind might not be a computational process.
I do not agree with this point of view. In this presentation, I attempt to explore possible
representations and transformation processes associated with the brain in order to defend
the concept of natural computation. I draw insights from David Marr’s theory of
vision—particularly his work on early visual representations—and from recent work in the
philosophy of mind, especially Andy Clark’s theory of predictive processing. The result is the
view that the mind (as instantiated in the brain) is a computational device, although perhaps
not a traditional one—it may lack a symbolic level.
Page 53Non-human error, the sublime, the beautiful, and the
grotesque in postdigital culture: an approach from the
philosophy of computing and aesthetics
Authors:
MARLENI REYES Monreal, Benemérita Universidad Autónoma de Puebla
Topic: Aesthetic Issues in and of Computing
Keywords: Computacional esthetics, glitch, non-human error
The expansion of computing into all areas of cultural life has generated new forms of
perception, creation, and aesthetic reception. Categories such as the sublime or the
beautiful are no longer understood solely through nature or traditional art; they now include
computational systems, algorithmic processes, and artificial intelligences. In postdigital
culture, error, noise, glitch, or decomposition appear as aesthetic expressions in themselves.
This research aims to contribute to a critical dialogue between artistic practice, aesthetics,
and the philosophy of computing, offering new perspectives on the criteria by which we
define what is aesthetically significant in our time. Computing mediates the aesthetic and it
acts as an ontological agent that radically transforms the meanings of the sublime, the
beautiful, and the grotesque, shifting their origin from nature or subjectivity to algorithmic
systems and technical infrastructures. This study seeks to analyze how computational
technologies reshape conceptualizations of aesthetic categories, through three sections: A)
a critical philosophical analysis of the categories, B) a comparative case study of
digital/glitch artworks, C) an analysis of the computational medium.
Page 54Noxæ: A Neuroethical Framework for Morally Adaptive AI
Agents
Authors:
Jesus Omar Lara Arriaga, 1
Topic: Frontier Artificial Intelligence, Neurocomputation, and Computational Linguistics
Keywords: brodmann areas, ethical AGI, neuroethical computing, quantum state collapse
As  artificial  intelligence  systems  increasingly  operate  in  ethically  sensitive
domains—autonomous warfare, policy-making, law enforcement, and smart cities—the
challenge arises: how can we create AI that makes morally sound decisions under
constraints of time, information, and computational resources? We present Noxæ , a novel
framework  combining  quantum  computation,  neurobiological  inspiration,  and  ethical
philosophy  to  meet  this  challenge.
At the heart of Noxæ is the Quantum Ethics Engine (QEG) , which encodes moral
decisions as quantum states in a high-dimensional Hilbert space. Ethical dilemmas are
represented as quantum superpositions, with multiple potential outcomes weighted by
stakeholder interests and contextual factors. These superpositions collapse to a defined
ethical eigenstate through unitary transformations, providing a morally coherent final action
despite uncertainty or incomplete data.
The framework is inspired by brain regions associated with ethical reasoning, such as
Brodmann Areas 4, 6, 9/46, and 44/6. It integrates embodied motor-intentional processes
with  complex  moral  deliberation,  grounding  decision-making  in  physical  and  ethical
navigation.
This represents a philosophical departure from traditional AI ethics, which rely on rule-
based or utilitarian approaches. Instead of deterministic calculations, Noxæ models moral
decisions as quantum phenomena, reflecting the complexity and uncertainty inherent in
ethical reasoning while maintaining computational efficiency.
Noxæ offers a scalable solution for morally adaptive AI, capable of navigating complex
moral landscapes while adhering to the primary directive: preserve human integrity
above all else . This framework is directly applicable to critical domains like autonomous
warfare, policy-making, law enforcement, and smart cities, providing a new pathway to
ethically aligned artificial general intelligence (AGI).
With Noxæ, we unlock the intersection of quantum mechanics, ethics, and AI, paving the
way for systems that can operate safely and morally in the most sensitive and high-stakes
environments.
Page 55Personal Assault, Property Damage, and the Hypothesis of
Extended Cognition
Authors:
Hector Omar Ruiz Rivera, University of St Andrews/University of Stirling
Topic: Ethical Issues in and of Computing
Keywords: Personal Assault; Ethics of the Extended Mind; Extended Cognition;
According to the extended mind hypothesis, originally proposed by Clark and Chalmers
(1998), non-neural objects in our environment, under the right conditions, can become part
of the mechanisms of our cognition. Proponents of this hypothesis argue that one of its
advantages is that it might align more closely with our ethical judgments regarding the
seriousness of the harm involved in intentionally affecting the environmental resources that
can be part of the realisers of cognition.
In particular, Palermos and Carter (2016) argue that, just as intentional damage to the
neural mechanisms responsible for cognition is considered a form of personal assault,
intentional damage to environmental objects should be considered in the same way,
provided those objects are part of the mechanisms of our cognition. They refer to this kind
of harm as "extended personal assault", a notion meant to capture the seriousness of
damaging tools that extend our cognition.
I will argue that defenders of this idea face a dilemma. If the added moral significance of
extended personal assault is related to the adverse effects of the harm, then this kind of
assault would merely be a terminological consequence of the extended mind hypothesis.
This is because property damage can involve the same adverse effects as damaging objects
that extend cognition. On the other hand, if the added significance of extended personal
assault is grounded in the idea that objects which extend cognition should be more
stringently protected than those that do not, then the notion relies on a mistaken idea about
normativity, since—as I will argue—such an idea is arbitrary. The conclusion, then, is that
proponents of the extended mind hypothesis do not, in fact, hold an advantage over
internalists when it comes to our ethical judgments about personal assault.
Page 56Por un uso reflexivo de la computadora en las ciencias
sociales
Authors:
Ángel Gutiérrez Escobedo, Universidad de Guanajuato
Topic: Ethical Issues in and of Computing
Keywords: Ciencias Sociales, Decisiones Teóricas, Software, Teorizar, Ética
Las ciencias sociales hacen uso del computador en distintos  momentos, incluso sin ser
capaces de reconocerlo . Esta condición es actual a la forma en que se produce conocimiento
científico y nos lleva a problematizar sobre las decisiones teóricas y metodológicas que
realizan los científicos  sociales,  sobre aquello que deberán declarar como resultado de dicha
relación.  La reflexión que se presenta, es en este caso, es una relación teórica con el uso de
los alcances computacionales para la teorización.  Cons ideraciones tales como la propiedad
del software, el uso de licencias libres o de  ciertos  navegadores  que tiene una reincidencia
la investigación , son dejadas de lado o puestas en términos de problemas externos, asuntos
accesorios que se asumen  como decisiones de índole pragmáticas o pre-teóricas, cuando
también son momentos de teorizar.  Es necesario discutir con este criterio pragmático para
para incrementar el uso de reflexiones éticas en los apartados teóricos-metodológicos, que
permitan hacer visibles, mediante  declaraciones,  tanto los compromisos  que surgen con el
uso de dichas tecnologías, como su posibles adecuaciones, esto con la finalidad de 
fortalecer la práctica, desarrollo de las ciencias sociales y su conectividad con otras
disciplinas científicas.    
Page 57Presentación del libro: Inteligencia Artificial. Enfoques
multidisciplinares.
Authors:
Karen González Fernández, Universidad Panamericana
Topic: Philosophy of Computing and Computer Science
Keywords: filosofía, ingeniería, inteligencia artificial, libro
Presentación del libro: Inteligencia Artificial. Enfoques multidisciplinares. 
Este libro contiene una compilación de artículos que tratan diversos problemas relacionados
con el desarrollo de la Inteligencia Artificial desde varias disciplinas: ingeniería, filosofía de
la ciencia, filosofía política, historia y filosofía de la IA, filosofía social, problemas lógicos de
la IA, derecho.
El libro busca ofrecer una presentación de algunas cuestiones problemáticas del desarrollo
de la Inteligencia Artificial, desde una perspectiva multidisciplinaria.
Datos bibliográficos del libro: Karen González Fernández y Alicia Mercado (comps.) (2025).
Inteligencia Artificial. Enfoques multidisciplinares. EUNSA.
Page 58Propaganda Detection on Twitter Mexico using Large
Language Model Meta AI
Authors:
Carlos Adolfo Piña García, Universidad Veracruzana
Topic: Gender, Politics, and Society in Computing
Keywords: ICL, LLM, LLaMA 3, Misinformation, Propaganda, Twitter
This study explores how Large Language Models (LLMs) can be used to detect political
propaganda on social media, using the 2018 Mexican presidential election as a case study.
By applying a few-shot prompting method within an In-Context Learning (ICL) framework,
we utilized a locally deployed version of Meta AI’s LLaMA 3.2 model to classify over 800,000
tweets as propagandist or non-propagandist.
A representative training subset of 7,858 tweets was used to guide the model’s behavior
through curated prompts. Propagandist content was defined by the presence of emotive
language,  and  adversarial  framing.  The  model  achieved  consistent  classification
performance across 100 evaluation rounds, identifying that 58.4% of the tweets exhibited
propagandist traits. These were characterized by negative sentiment, aggressive tone, and
strategic hashtag usage.
We carried out an additional sentiment and clustering analyses to evaluate the emotional
patterns and coherence within the classified tweets. Notably, Rosengren’s Dispersion
Analysis revealed how certain high-frequency terms (e.g., “amlo”, “presidente”) are broadly
used, while emotionally charged terms appear in narrower contexts, indicating coordinated
messaging tactics.
This research shows the utility of LLMs in social network analysis, offering a replicable
method to identify manipulation in social media narratives. Our framework is suitable for
real-time or localized deployment scenarios.
This study contributes to ongoing efforts to develop automated, AI-assisted misinformation
detection on social networks for sensitive political events.
Page 59Quantum computing and AGI: a theoretical-philosophical
analysis for the quest of superintelligence.
Authors:
Edmar Soria, Universidad Autónoma Metropolitana
Topic: Theory of Computation, Formal Languages, Logic, and Foundational Mathematics
Keywords: Quantum Computing, Quantum Convolutional Neural Networks
The primary aim of AGI research is to design computational systems that emulate, match or
exceed human cognitive-consciousness capabilities across all domains of interest, and for
that an extensive multi-interdisciplinary endeavour has been developed several decades
ago. In the particular context of Philosophy of AGI and the so called, Technological
Singularity, one of the main objects of discussion is the concept of superintelligence and 
ultraintelligent machine , which implies a profound research not only in technological
development or theoretical mathematics, but in the core concept of humanity itself from an
ontological and even a metaphysical perspective. In parallel, quantum computing seeks to
exploit superposition and entanglement to achieve algorithmic speed-ups and tackle large,
complex problems that are intractable on classical hardware. And with the emerging field of
Quantum Machine Learning, the convergence between Ai and Quantum Computing is
beginning to open some speculative and philosophical perspectives about a core question: 
 
Can Quantum Computers Contribute to the Emergence of Superintelligence? 
 
This conference will provide a brief state of the Art analysis in Quantum AI / Quantum
Machine  Learning  research,  alongside  with  a  brief  review  of  the  basic  theoretical
foundations of AGI concepts of superintelligence  and  ultraintelligent machine, in order to
provide a conceptual framework for understanding recent discussions about the feasibility
and challenges of the role of quantum computing for contributing to the emergence of
super-ultra intelligence, taking into account key restrictive observations such as the
Bekenstein bound, the Bremermann’s limit, and the Holevo’s theorem. Derived from this
framework, the talk will end with the original proposal of a philosophical discussion of a
speculative design of the Darwin-Godel Machine with quantum capabilities.
Page 60Reflexiones filosóficas sobre el uso de herramientas de
Inteligencia Artificial en la escritura de textos académicos
(tesis y artículos científicos)
Authors:
Karen González Fernández, Universidad Panamericana
Topic: Ethical Issues in and of Computing
Keywords: escritura híbrida, inteligencia artificial, investigación científica, plagio
A finales del siglo XX, con la aparición del internet y, posteriormente, con la aparición de las
herramientas computacionales capaces de procesar grandes volúmenes de información, la
investigación científica se vio revolucionada.
El día de hoy contamos con la mayor cantidad de información de toda la historia de la
humanidad disponible para ser consultada, y contamos con herramientas computacionales
que  pueden  ofrecer  búsqueda  de  fuentes,  hacer  traducciones,  corregir  redacción  y
ortografía, y hasta escribir textos argumentativos. ¿Qué implicaciones tiene esto para la
investigación científica?
En esta propuesta propongo reflexionar sobre dos bloques de problemas: a) los problemas
de plagio y la escritura híbrida; y b) la posibilidad de plantear la obligatoriedad del uso de
herramientas de inteligencia artificial para hacer investigación científica.
A partir de las discusiones generadas sobre ambos bloques de problemas, concluiré con
algunas reflexiones sobre el papel que instituciones como las universidades, los centros de
investigación y los Estados mismos, juegan y deberán jugar en la integración de las
herramientas computacionales al trabajo de la investigación científica.
Page 61Relational Perspectives and Algorithmic Biases in the Use of
Artificial Intelligence in Educational Contexts
Authors:
Manuel Soto-Romero, Facultad de Ciencias, UNAM
Karla Ramírez-Pulido, Facultad de Ciencias, UNAM
Ana Cristina Cervantes-Arrioja, Facultad de Ciencias, UNAM
Topic: Gender, Politics, and Society in Computing
Keywords: Education, Ethics, Gender, Generative Artificial Intelligence
This paper analyzes the functioning of Generative Artificial Intelligence (GAI) and its
application in educational contexts, highlighting its potential to personalize teaching,
automate teaching tasks, and optimize institutional management. At the same time, it warns
of gender biases that may be reproduced or amplified through these technologies, resulting
from non-neutral data and sociotechnical design decisions. A methodology based on
multilingual benchmarks and a survey applied to women is proposed to critically assess
these biases, with the aim of promoting an ethical and responsible use of GAI in education,
centered on equity, critical thinking, and knowledge verification. Finally, a series of
recommendations are proposed to help analyze the situation from multiple dimensions,
fostering critical, ethical, and contextual thinking in technoscientific societies, emphasizing
the regulated use of GAI under a conceptual framework that mainstreams a gender
perspective and a methodology oriented toward detecting biases in technologies.
Page 62Relational Theory of Machines and the Hardware/Software
Distinction: a First Approach toward an Adequate Ontology
Authors:
Enrique F. Soto-Astorga, UNAM
Topic: Philosophy of Computing and Computer Science
Keywords: Hardware/Software, Ontology of Computing, Philosophy of Computing, Relational Science,
Robert Rosen
This paper problematizes the assumed hardware/software dualism through Robert Rosen’s
philosophical work. After revisiting the foundations of Turing machines and the physico-
axiomatic extensions by Gandy and Sieg, it is argued that such models depend on the
externality of the transition rule and the finite assemblability of the device. Thus, Robert
Rosen’s Relational Theory of Machines is introduced, redefining machines by distinguishing
between causally effective components (hardware) and inert components (software), and
highlighting the existence of systems in which this distinction collapses. It concludes that
the  relational  perspective  invites  us  to  rethink  the  assumptions  underlying
pancomputationalist programs (particularly the supposition that the Church-Turing-Post
thesis is natural law).
Page 63Resilient Epistemic Environments in the Age of AI
Authors:
Alejandro David Tamez, Center for Cyber-Social Dynamics at the University of Kansas
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Artificial Intelligence, Epistemic Environments, Epistemic Injustice, Social Dynamics
This paper argues that the increasing integration of artificial intelligence (AI) systems into
public life demands a shift in how we understand and protect epistemic agency. I defend
two  central  claims.  First,  generative  technologies  such  as  deepfakes,  algorithmic
governance,  and  automated  surveillance  significantly  destabilize  the  epistemic
environments in which individuals and institutions operate. Second, adequate responses to
these disruptions require a social rather than individualistic epistemology.
I begin by developing the concept of epistemic environments , distinguishing it from
epistemic  infrastructures.  While  infrastructures  refer  to  deliberately  built  knowledge
systems like universities and courts (Malazita 2020; Bandola-Gill 2022), environments
include both structured and informal sites of epistemic activity, such as digital discourse
and everyday communication. Drawing on Aristotle’s political philosophy, I argue that just
as moral development depends on one’s political environment, epistemic agency depends on
the conditions of one’s epistemic environment.
Next, I show how AI technologies undermine these environments. Deepfakes, for example,
erode the testimonial function of digital recordings (Rini 2020), while the growing public
awareness of AI-generated media produces what Chesney and Citron (2019) call the liar’s
dividend which refers to occasions when even genuine evidence can be dismissed as fake.
To address these challenges, I turn to social epistemology. Drawing on Frost-Arnold (2022)
and Goldberg (2012), I argue that epistemic resilience depends on shared institutions,
norms, and trust practices rather than solely on individual reasoning. I propose a framework
grounded in social epistemology for evaluating and strengthening these environments.
This paper contributes to current debates in the philosophy of computing by articulating
how epistemic environments can be made resilient in the face of technological disruption,
and what is required to sustain trust and knowledge in an AI-mediated world.
Page 64Robots as Religious Entities: Techno-Philosophical-
Theological foundations for Machinic Religiosity and Artificial
Spirituality.
Authors:
Edmar Soria, Universidad Autónoma Metropolitana
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: AI spirituality, Apocalyptic AI religiosity, Escathological AI, Robots and Religion
This  research  explores  the  possibility  of  synthesizing  perspectives  from  technology,
philosophy of AI, theology, and the science of religion in order to study the potential for
machines and AI to achieve religiosity and spirituality through the  addressing of two
central  themes:  a)  the  multiple-multidisciplinary  conditions  necessary  for  robots  to
authentically engage as religious agents themselves, and b) an analysis of "Apocalyptic AI"
foundations, examining their eschatological underpinnings and consequent implications for
human-machine relations and AI spirituality.
Page 65Semantics of Thought Experiments
Authors:
CP Hertogh, Chongqing University, VUB Brussels
Topic: Theory of Computation, Formal Languages, Logic, and Foundational Mathematics
Keywords: (extended) argument view, (extended) logic view, (non)classical logic, descriptive semantics,
possible worlds semantics (PWS), progress of science and society, semantics, thought experiment (TE)
The research question of Semantics of Thought Experiments (TE) for a unified but
nonreductionist theory of TE is answered by a provisional proposal involving four views,
Extended Argument View (TE Matrix, TE Diagram), Extended Logic View (incl. plausibility
logic, possible worlds semantics PWS), Descriptive Semantics View, and Progress of Science
and Society View (incl. global cross-culturalism and environmental pragmaticism).
For the skeptics there are proposed Transformation Rules or Substitution Theses to
substitute TE by experiments (TR/ST1) and nonmodal arguments (TR/ST2).
The semantic TE theory is successfully applied to over ten examples of TE from
mathematics, philosophy of mind (consciousness studies) and philosophy of natural sciences
(classical and relativity physics) with help of TE Matrix, a TE specific logical notation and
procedure, involving bracketing of TE from [TE]RS, [TE]BS to [TE]EX (resp. restricted, broad,
extended TE arguments) until TE have been fully developed into valid and sound formal
logical arguments.
Page 66Semillas de hiperstición: arte especulativo y tecnociencia
vitalista como impulsores de futuros posibles
Authors:
Angel Francisco Flores Ayala, Hypatia
Ana Laura Ortiz Astudillo, irr4ti0n4L kawaii
Topic: Aesthetic Issues in and of Computing
Keywords: Arte Especulativo, Cultura Cibernetica, Estetica Digital, Futurología Critica, Hiperstición,
Solarpunk, Tecnofeminismo
Esta ponencia propone una reflexión crítica sobre el papel del arte como agente activo en la
configuración de futuros posibles, en un escenario marcado por la crisis ecológica, el
colapso epistémico y la intensificación del tecnocapitalismo. A partir de una relectura
situada de la noción de hiperstición —entendida como narrativa ficcional con capacidad
performativa sobre lo real— se articula un marco conceptual que combina futurología
crítica, tecnociencia vitalista e imaginarios ecocéntricos.
Se  plantea  que  ciertas  prácticas  artísticas  contemporáneas  operan  como  semillas
hipersticiosas , es decir, ficciones especulativas que provocan desplazamientos sensibles y
políticos en el imaginario colectivo. En este contexto, se analiza el trabajo de una colectiva
artistica  y un proyecto ficción especulativa ,  cuyas acciones articulan la creación de
imaginarios como herramientas de reapropiación crítica.
Desde una perspectiva latinoamericana, se sostiene la especulación como táctica de
resistencia y supervivencia. Así, el arte se concibe como laboratorio de futuros, donde
imaginar no es predecir, sino intervenir en lo posible.
Page 67Some Principles for Formalizing Causality in Computer
Science and the Logic of Causal Implication by R. Sylvan and
N. da Costa
Authors:
Ramazan Ayupov, Research Assistant: International Laboratory for Logic, Linguistics and Formal
Philosophy (HSE University)
Topic: Theory of Computation, Formal Languages, Logic, and Foundational Mathematics
Keywords: Causality, Sylvan, computer science, counterfactuals, da Costa, non-monotonic logics, relevant
logics
This report addresses the concept of causation in its connection with formal systems, as well
as the logic of causal implication (CI) by Richard Sylvan (Routley) and Newton da Costa.
This logic is founded on a novel connective, A \ni  B ("A causes B"), and a temporal, strict
linear order ("to be earlier than"). The logic is constructed upon the foundation of relevant
semantics.
In the discourse surrounding the nature of causality, David Lewis's logic of counterfactuals
and the approach of Joseph Halpern, where causality is formalized using structural
equations, have gained the most prominence. The work of Sylvan and da Costa presents an
alternative framework for the formal examination of causation. The causal conditional \ni is
a  connexive,  strict  relevant  implication  that  satisfies  the  properties  of  transitivity,
irreflexivity, Modus Ponens, and Modus Tollens. It also adheres to Aristotle's thesis,
Strawson's rule, the principles of composition, and the principle of Augmentation. However,
it does not comply with the principle of identity, contraposition, simplification, addition,
substitution, or the factorization principles (even in a restricted form). Furthermore, it is
non-monotonic and does not permit the principle of explosion.
The report examines the primary requirements for causality within the context of their
application in various formal systems and computer science, as well as the language,
semantics, and axiomatization of CI logic. It also analyzes and compares the fundamental
properties and principles of CI logic with Lewis's logic of counterfactuals and provides a
taxonomy of causes based on their objectives.
 
Page 68Speaking algorithms: on the limits of incomputability in
Human–Machine Communication research
Authors:
Sara Barrios Rangel, Grupo de Investigación en Filosofía de la Computación. UNAM
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Human–Machine Communication, communication science, incomputability, philosophy of
computation, technocapitalism, technoscience
The integration of various artificial intelligence (AI) technologies into information and
communication technologies (ICT) for communicative and conversational purposes, along
with the proliferation of intelligent social agents, has opened new research frontiers in the
field of communication studies. These developments also demand a critical re-examination
of the ontological and epistemological assumptions that have historically shaped the
discipline.
In this context, Human–Machine Communication (HMC) emerges both as a new research
area and as an evolving conceptual framework. HMC posits that machines should not
merely be viewed as mediators, but as interlocutors and producers of meaning in
communicative processes, whether interpersonal or mass mediated.
This presentation aims to analyze the philosophical foundations of HMC through the lens of
the  philosophy  of  computation  and  the  concept  of  incomputability.  It  explores  the
ontological assumptions and implications of treating machines as communicative subjects
and meaning-makers, particularly within the context of technoscientific culture and the
development of AI technologies that commodify affect under the logic of technocapitalism.
Topic: The Uncomputable in Other Disciplines
 
 
Page 69Technological adoption in the production of artisanal textiles
in Mexico
Authors:
Carla Espinosa Herrera, Universidad de Guanajuato
Topic: Aesthetic Issues in and of Computing
Keywords: Arte, Computation, History, México, Textil
Mexico is internationally recognized for its artisanal textile production, a cultural identity.
The  practice  of  this  textile  reflects  the  worldview  and  adaptability  of  its  people.
Economically, we can emphasize a number of other factors, such as technology and its
ancestral production process. The objective of this analysis is to trace the technological
history of the country to link technological adoption to Mexican artisanal textile production,
highlighting the connection between the history of computing and looms, the profound
cultural transformations that accompany these changes, and the fundamental role of art as
a way of being for Mexicans, a means of overcoming the complex issue of identity in a
globalized world.
The reality of this technological era in which humanity lives is closely linked to a digital or
computerized culture; however, it has involved a process that some generations have
experienced with greater contrast. Therefore, it is surprising for many to recognize the
intrinsic link between textile machinery and the dawn of computing. Before computers as
we know them existed, programmable looms already laid the foundations for automation
and information processing. With this foundation in mind, we will address the technological
adoption and innovation in this textile process, which we will address as Mexican art as
another opportunity for innovation and human development.
Page 70Technology development education in Mexico based on a
human rights approach
Authors:
Ricardo Castillo Solano, PhD student at Universidad Iberoamericana Ciudad de México
Topic: Gender, Politics, and Society in Computing
Keywords: Human rights approach, education, engineering, technology development
Through an exploratory theoretical research, employing documentary and observational
techniques, it was concluded that, as of 2024, the ten highest-ranked universities in Mexico
(according to the QS World University Rankings) do not offer, within their engineering
schools or faculties, an academic program or course focused on the development and
creation of technology from a human rights-based approach.
In light of this gap, and in order to safeguard human dignity and the development of
individual personality both online and offline, it is imperative to integrate the teaching of
general human rights obligations (protection, promotion, guarantee, and respect), as well as
specific  duties  (sanction,  reparation,  investigation,  and  prevention)  into  engineering
education.
The aim is to ensure that those who create technology understand the scope and limits of
human rights protection and, where applicable, the consequences of the misuse or harmful
development of technology. Ultimately, this seeks to evolve technology ethics into a binding
duty—one that should be adopted by all States through their educational systems.
Page 71The Contextual-Irruptive Forecasting Model (CIFM)
Authors:
Alina Y. Hernandez-Porrello, AgileEngine LLC
Enrique F. Soto-Astorga, Facultad de Ciencias - UNAM
Topic: Philosophy of Interaction Design
Keywords: anticipation, computational systems, contextual variables, subjective experience, user cognition,
user-system interaction
The Contextual Irruptive Forecasting Model (CIFM) proposes a novel framework for
understanding  and  anticipating  user  cognition  during  interaction  with  computational
systems. Grounded in a relational and temporally sensitive view of user-system engagement,
CIFM analyzes the dynamic interplay between two categories of variables: contextual
variables,  which  are  relatively  stable  or  gradually  evolving  aspects  of  the  user’s
environment such as time, location, weather, long-term goals, and preferences, and
irruptive variables, which include disruptive events such as notifications, emotionally
significant incidents, or interruptions.
By  treating  these  variables  as  structurally  distinct  yet  co-constitutive  of  cognitive
experience, CIFM seeks to infer shifts in a user’s cognitive state through computationally
tractable, objective measures. In doing so, it challenges traditional models that presume
static or rationally consistent users, and instead aligns with a more nuanced, interactional
epistemology that foregrounds discontinuity, affect, and context.
Philosophically, CIFM contributes to ongoing discussions around the nature of subjectivity,
attention, and anticipation in digital environments. It invites a rethinking of how cognition is
modeled, not as a self-contained process, but as something emergent from lived and
interruptible experience. This approach enables the development of computational models
that aim for attunement rather than mere prediction, opening pathways toward ethically
aware and context-sensitive adaptive systems.
In addition, CIFM offers UX design a novel conceptual and practical tool to objectively
measure subjective experience. By operationalizing disruptions and contextual shifts as
data-rich signals of internal state transitions, the model provides designers and researchers
with a framework for interpreting user engagement not solely through performance metrics,
but through the ebb and flow of experiential and affective dimensions.
This presentation will elaborate the philosophical foundations of CIFM, its methodological
architecture,and its implications for the design of interactive systems that forecast in ways
responsive to the complexity of being human.
Page 72The Feeling Machine: Aesthetics, Algorithmic Agency and the
Future of Audiovisual Editing
Authors:
Andrea Carolina Camacho Yáñez, Corporación Unificada Nacional de Educación Superior CUN
Luis Fernando Gasca Bazurto, Corporación Unificada Nacional de Educación Superior CUN
Topic: Aesthetic Issues in and of Computing
Keywords: Aesthetic, Algorithmic Agency, Audiovisual Editing, Feeling Machine, Future AI
The expansion of artificial intelligence in creative processes has profoundly
transformed the language of audiovisual montage, seemingly shifting human sensibility
into a shared hybrid terrain: sentience and algorithms. This paper proposes to reflect,
from a philosophical and aesthetic perspective, on how AI reconfigures the sensitive
space -following Jacques Rancière, the sharing of the visible, the decipherable and the
thinkable- by intervening in the production of visual narratives supported by tools such as
Virbo AI. From the practical exemplification with historical archives, we will analyze the
aesthetic, ethical and philosophical implications of delegating to the algorithm the
animation, recontextualization and narrative construction of images,questioning how they
are
transformed in an algorithmic environment where documentary authenticity is altered by
logics of visibility learned from contemporary datasets, imposing a globalized aesthetic
that tends to homogenize cultural and temporal diversity. It is proposed that AI actively
participates in the creation of meaning by redistributing the sensitive and generating new
forms of perception, temporality and visual memory. In this intersection between aesthetics,
technology and archive, we ask ourselves in dialogue with Turing (1950), if we are ceding
the control of creation to the machine and what risks this exchange implies in the
construction of collective memory and authenticity, since "we may agree that machine
equals brain, that is, not only that it writes it, but that it knows that it wrote it" (Jefferson,
1949). Finally, we address how AI introduces innovations in audiovisual montage -
impossible transitions, compelling anachronisms and new forms of visual language - while
posing risks of aesthetic homogenization, cultural biases and damage to collective memory
(Crawford, 2021; Fernandez, 2024). We propose, then, that the future of audiovisual
montage will necessarily be hybrid, redefining creative agency, perception and visual
memory in the age of artificial intelligence.
Page 73The Hybrid Ontology of Digital Normativity:
Reconceptualizing Legal Authority in Computational
Environments
Authors:
Johanna Mildred Pinto García, UNAM
Topic: Gender, Politics, and Society in Computing
Keywords: Computational jurisprudence, digital governance, information ontology, lex digitalis
The Hybrid Ontology of Digital Normativity: Reconceptualizing Legal Authority in
Computational Environments
This investigation advances a sophisticated ontological framework for understanding lex
digitalis—the  autonomous  normative  architectures  governing  contemporary  digital
platforms. Deploying Floridi's information ontology as the primary theoretical apparatus,
the analysis conceptualizes digital norms as hybrid informational objects that transcend
conventional  jurisprudential  categorizations  through  their  simultaneous  computational
codification and social validation.¹
The central thesis establishes that platform governance mechanisms operate as relational
informational entities within computational environments, achieving juridical authority not
through traditional institutional sovereignty but via their capacity to regulate behavioral
patterns while configuring social reality. This ontological hybridity fundamentally challenges
established legal theoretical frameworks, necessitating reconceptualization of authority in
algorithmically mediated environments.
Hildebrandt's computational jurisprudence provides the normative architecture, specifically
her legal protection by design framework, which demands architectural embedding of
constitutional principles rather than retrospective regulatory imposition.² The COVID-19
disinformation regulation serves as the empirical demonstration, revealing how platform
algorithms functioned as epistemic arbiters that configured public discourse through
computational processes rather than deliberative democratic mechanisms.
The analysis yields strategic regulatory pathways: architectural constitutionalism that
embeds  rule  of  law  principles  within  technological  infrastructure,  transnational  co-
regulatory mechanisms integrating platform autonomy with democratic accountability, and
algorithmic transparency frameworks enabling meaningful contestability. This contribution
establishes theoretical foundations for digital constitutionalism while providing actionable
frameworks  for  platform  governance  that  preserve  democratic  legitimacy  within
computational  environments.
References:
Luciano Floridi, The Philosophy of Information  (Oxford: Oxford University Press, 1.
2011), 78-92.
Mireille Hildebrandt, Smart Technologies and the End(s) of Law  (Cheltenham: Edward 2.
Elgar, 2015), 245-267.
Page 75The impact of personalized algorithmic sorting on digital
epistemic systems
Authors:
Brenda Areli Figueroa Ahumada, UNAM
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Epistemic networks, algorithms, digital communities, misinformation, social epistemology
By analyzing the digital ecosystem as an epistemic system, it is easier to explain the impact
that personalized algorithmic sorting systems have on the creation and propagation of bad
beliefs. In contrast with individualistic approaches to the subject, this view will explain the
way the digital ecosystem promotes and rewards anti-epistemic practices regarding of
education level or good intentions.
Bad beliefs, as defined by Neil Levy, are beliefs that aren't true yet are rationally obtained
by those who hold them and cannot be changed even when confronted with evidence
against them. These beliefs play a significant role in the agent's social life, as they are often
beliefs about a certain shared reality between peers.
The rise of content that promotes bad beliefs is intrinsecally linked with the proliferation of
profiles that are in turn used to personalize the information people have within a digital
platform. The transmission of this content is done by algorithmic sorting that priorize
engagement, such as comments, reblogs and reactions, over factuality and truth.
This is done in order to maximize both traffic and user captivity that translates into
advertising  revenue.  Adverstising  companies  then  utilize  this  technology  to  design
misinformation campaigns. Whats even worse, institutions such as scientific, educational
and journalistic communities, are also damaged as pre-existing harmful practices such as
yellow journalism, paid and biases scientific research and school bullying, are accelerated
and rewarded by the algorithm classification system.
The epistemic damage comes from the manipulation of the transmission of information, as
whoever owns the algorithmic systems, has the power to censor, twist or amplify what is
seen by which users. This power extends to the creation and segregation of online
communitiesm effectively cutting the flow of information in what was sold as a free reign
space.
Page 76The Oath to Janus: Computational Ethics for people in STEM
in the Age of AI and Technocracy
Authors:
Alejandro Axel Rodríguez-Sánchez, UNAM School of Sciences
Topic: Ethical Issues in and of Computing
Keywords: Data governance, Ethics of computing, STEM
Drawing from Luciano Floridi's vision of IT professionals as "priests of Janus", where like
the Roman god of thresholds, they possess the rare ability to peer through the interfaces
that remain opaque to most of humanity; this work argues that STEM professionals,
specially computer scientists, have received a supra-divine status as result of computer
sciences ascending as the main carrier of the positivist scientific program legacy.
However, unlike other disciplines with similar status like medicine, computing lacks
humanistic, moral and ethical compasses. This is not a trivial issue in contemporary reality
architecture, as computing professionals find themselves as unwitting architects of a new
epistemic order where algorithmic cognition determines our very conception of what it
means to know, to choose, and to be.
I identify two obstacles preventing the establishment of these guidelines: First, the dualist
distancing  of  the  so-called  hard  sciences  and  humanistics,  making  these  seem  as
unnecessary. And second, the capitalist patronage of computing, which renders them not
only unnecessary but undesirable.
Finally, I elaborate that computing systems development can benefit from the inclusion of
ethics into its toolbox. To motivate the overcoming of these burdens, I propose a practical
framework which I name as "The Oath to Janus", for addressing this responsibility,
consisting of four core principles: model humility, historical recognition, algorithmic justice,
and sacred privacy and cognition. These emanate from a political review of computing and a
critique from novel and challenging scientific theories, like Robert Rosen's Anticipatory
Systems.
Page 77The power of interaction in computing: A philosophical
Primer
Authors:
Miguel Angel Andrade, UNAM, School of Sciences
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Interactive Computing, Metascience, Multiparty Computing, Philosophy of Science
Ever since the inception of modern computing, interaction has been theorized as a means to
supercharge what machines are able to do. The power of this paradigm of computing has
never been as evident as in today’s world, where distributed systems, user interfaces and
cloud computing are the norm. Furthermore, more novel approaches such as Multiparty
Computation and Interactive Turing Machines appear to be better than the traditional
models for certain types of tasks.  The aim of this talk is to explore the philosophical
underpinnings of this idea and show that interaction provides a better ground for computing
than traditional sequential computing. Not only are the considerations practical, i.e. some
computing paradigms such as networks are more intuitive in this framework, but also is a
firm epistemic stance on what computer science can mean. Additionally, I will briefly
discuss what implications this has for not only the future of computing, but also what it
means for other areas of science.
Page 78The prospect of automating mathematical discovery:
Challenges in the use of AI in mathematical proofs
Authors:
Nancy Abigail Nuñez Hernández, FES Acatlán, UNAM
Topic: Philosophy of Computing and Computer Science
Keywords: Artificial Intelligence, automation, computational complexity, mathematical proofs
Proof discovery in mathematics has been the pride of mathematicians since the discipline's
early days. Due to the crucial role of proofs in mathematics, various values have been
attributed to them: proofs have been described as having explanatory, aesthetic, or
motivational value. But even if mathematical proofs are so valuable, proving a mathematical
statement is not easy. The history of mathematics and the experiences of mathematicians
attest to the difficulty of discovering proofs. Many seem to believe that the progress of
Artificial Intelligence (AI) will be a game changer, enabling the automated discovery of
mathematical proofs and making the task of proving mathematical statements easier, if not
effortless. But is it actually possible to fully automate the discovery of mathematical proofs? 
This talk aims to explore this question, taking into account computational complexity
constraints.
Page 79The question of lucid dreams
Authors:
Gerardo Islas Escobar, Student at UNAM
Topic: Computational and Non-Computational Cognitive Science
Keywords: consciousness, dreams, lucid dreaming, sleep
The existence of dreams poses intriguing questions regarding consciousness. Various
attempts have been made to understand these phenomena, from spirituality to neuroscience
in recent years. Still, we have some blind spots that do not allow us to grasp what dreams
are. In this project, I try to begin to answer this question by, hopefully, setting a framework
based on the particular case of lucid dreaming, a singular occurrence of sleep. By doing
this, maybe at some point we will be able to extend the framework to general dreaming and
eventually consciousness.
For starters, I believe there is a material component to lucid dreaming, but it does not
explain the entirety of the phenomenon. This is the basis for developing the project. What is
beyond neural connections? By exploring the relations between neurons, and analyzing the
way they change depending on the inputs and outputs, perhaps this could help us
understand this key mystery for humanity.
But it is just a theory, a game theory.
Page 80The Senses, Algorithms, and Markets: Toward a Bayesian
Market of Behavior
Authors:
Luis Diaz, ENAH
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: accelerationism, algorithmic dynamics, ideological sign
This essay explores the relationship between ideological signs  and algorithmic dynamics
in the human mind. It forms part of a broader investigation into the philosophy of language
and  the  embodied  basis  of  perception.  Algorithms  influence  not  only  our  musical
preferences but also our film consumption, gaming experiences, and sense of immersion in
virtual environments. These processes are not neutral; they construct an ontological  field
manifested in identity, meaning, and behavior while feeding emerging markets such as
metadata, biometrics, and digital security, all of which are representative of an axiological
ethics.
We may justify security for banking and cuteness for kitty-faced avatars, but both serve as
ideological signs that legitimize new modes of production and digital rent economies. This
transformation has been referred to as accelerationism , where traditional ownership gives
way to experience-based access models. Drawing on Voloshinov’s theory of the ideological
sign and Bayesian models of behavior, this presentation questions whether algorithmic
governance leads toward deterministic subjectivities. It aims to open a discussion on how
sensation, identity, and power operate in digitally mediated societies increasingly shaped by
probabilistic logic.
 
Page 81The Uniquely Human Elements in the Work of a Clinical
Psychologist: What Artificial Intelligence Cannot Do
Authors:
Jesús Ignacio Rivera Cano, Psicólogo, Universidad de Antioquia. Especialista en Psicología, Universidad
de San Buenaventura. Magíster en Psicología, Universidad CES. Formación Pedagógica, Universidad
Autónoma Latinoamericana. Formación continua en Psicología, Universidad Nacional Autónoma de
México. Psicólogo clínico Hospital Mental de Antioquia.
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: Clinical psychology • Artificial intelligence • Phenomenology • Intersubjectivity • Embodied
empathy • Therapeutic ethics • Therapeutic relationship • Authentic presence • Technological limitations •
Technological complementarity
Este trabajo examina las dimensiones ontológicas irremplazables del encuentro
terapéutico humano en comparación con las capacidades de la inteligencia
artificial,  desde  una  perspectiva  fenomenológica  respaldada  por  evidencia
empírica. Se identifican siete pilares constitutivos de la práctica clínica humana:
empatía  encarnada,  intersubjetividad  dialógica,  juicio  ético  situacional,
creatividad terapéutica, presencia auténtica, tolerancia a la ambigüedad y
construcción narrativa compartida. Estas dimensiones se contrastan con cinco
dominios de limitación empíricamente documentados en los sistemas de IA:
ausencia de conciencia fenomenológica, incapacidad para procesar paradojas
emocionales,  sesgos  en  contextos  culturales  no  normativos,  rigidez  en
situaciones no protocolizables y ausencia de agencia moral. Argumentamos que
estas limitaciones no son meramente técnicas sino ontológicas, derivadas de la
irreductibilidad de la experiencia encarnada e intersubjetiva, sugiriendo un
modelo complementario donde la IA puede amplificar aspectos instrumentales
del trabajo clínico mientras que las dimensiones relacionales fundamentales
requieren presencia humana auténtica.
Page 82The university scientific process for research in intelligent
systems: a teaching reflection from a philosophical
perspective.
Authors:
Leticia Flores-Pulido, Universidad Autónoma de Tlaxcala
Topic: Philosophy of Computing and Computer Science
Keywords: artificial intelligence, basic science., hermeneutics, intelligent systems, research process
Intelligent systems use the scientific method to achieve computational intelligence projects,
but we sometimes ignore the process carried out by the main actor: the university student.
This document reflects on the scientific research process through which university students
acquire knowledge through hermeneutics as they journey through the thesis, article, or
programming  development,  abstraction,  and  implementation  of  artificial  intelligence
methods. The above aims to achieve an intelligent system that supports the generation of
knowledge, basic science, and cutting-edge innovation that scientific products can achieve.
This highlights the will to influence its impact on the solution of national problems, to
increase the productivity of professionals, and to improve the quality of life through artificial
intelligence from a philosophical perspective.
Page 83Thinking Without a Subject: AI and the Mutation of Thought
Authors:
Abraham Yebra Pimentel, Centro de Estudios Filosóficos Tomás de Aquino
Topic: Frontier Artificial Intelligence, Neurocomputation, and Computational Linguistics
Keywords: Artificial Intelligence, Cognitive Ontology, Epistemic Limits, Exteriorization of Thought,
Inorganic Ontology, Posthuman Reason, Reza Negarestani, Simondon, Technical Objects
The questions that concern us here are the following: is it possible to find a new form of
thought within the human–AI relationship? And can this relationship modify the very
structure of thought? We understand thought as a self-organizing process that, upon
receiving information, is not only affected by it but also modifies its own structure. While
experience has an influence on it, it doesn't fully determine it—thought organizes itself.
That’s why AI should not be seen as just another technical object, but rather as part of the
very system of thought. If we can show that AI participates in this self-organization, then we
can begin to speak of a real mutation of human thought. Simondon (1958) argues that
technical objects evolve and embody human cognitive processes; thought materializes in
them and transforms along with them. The example of the microwave shows how a new tool
reshapes our practices, and through that, reconfigures our mental frameworks. Technique,
then, doesn’t just respond to the human—it restructures them. If AI is an exteriorization of
thought that returns reorganized, then we’re facing something unprecedented: thinking
thought from the outside. AI would no longer be just a tool, but a distributed formal function
that displaces intelligence beyond its biological support, as Negarestani (2018) suggests. In
this sense, thought does not change merely through direct experience, but through its
reconfiguration in relation to AI. This is how the hypothesis finds resolution: if AI
externalizes, reflects, and reorganizes thought, then yes—the human–AI relation modifies
the structure of thought. And with that, a new form of thinking emerges—not organic, not
subjective, but transductive and distributed. Thinking is no longer just an internal
experience, but also an external operation.
Page 84Towards a complex theory of information
Authors:
Pedro Arturo Góngora Luna, unaffiliated
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: abstraction, information theory, metaphysics, ontology, phenomenology
We give the foundations for a complex theory of information. It is complex in the sense that
it accounts for both qualitative and quantitative aspects of information, therefore beholding
both subjective and objective informational phenomena. It is also complex in the sense that
approaches a layer of abstraction structuring information as such, thus, a layer meta-
structuring any kind of discourse. By its nature, abstractions made here through a
schematic approach, allow simple formal and graphical representations, coming with a rich
and novel top-down visual reasoning methodology grounded in a complex theory of duality.
On these grounds, our theory departs from the code-theoretical and algorithmic approaches
of Shannon and Weber, and Chaitin respectively, by instead adopting a phenomenological
and semiotic perspective.
Page 85Twin transition
Authors:
Salvador Garcilita Arguello, Bioalgoritmia https://bioalgoritmia.com/index.html
Topic: Gender, Politics, and Society in Computing
Keywords: Digital transformation - Sustainability - Twin transition
Humanity—and the entire biosphere—is facing an unprecedented global crisis. Climate
change, pollution, and biodiversity loss threaten the delicate balance of ecosystems,
societies, and life as we know it. In response, urgent calls for action have emerged: from
global frameworks like the UN 2030 Agenda for Sustainable Development, to ambitious
regional cooperation programs and local initiatives. Governments, industry, and academia
alike are being called upon to contribute with ideas, strategies, and actions that drive us
toward a more sustainable future—a green transition.
At the same time, we are living through an era of explosive innovation where computers and
algorithms,  which  have  never  been  more  powerful  and  accessible,  are  continuous
protagonists. In the era of what we call artificial intelligence, digital transformation has
never been more important for any government or industry. Whatever the line of business
is, data, automation and software will be there: a true digital transition.
But what happens when both transitions intersect? Are nature, life, and the mission of
protecting them incompatible with digitization, computing, and frontier technologies? Or
could this be a once-in-a-generation opportunity? And if so, are the right tools being applied
in the right way? What ethical and contextual considerations must we take into account?
This talk explores the motivations, challenges, and opportunities of integrating digital
technologies, computing and frontier algorithms into efforts to address the “wicked
problems” of climate change, biodiversity loss, and pollution, merging both digital and
green transformations into a single one: a twin transition.
Page 86Understanding with machines: situated practices of machine
learning
Authors:
Rinnette Riande González, Master’s Program in Philosophy of Science, UNAM
Topic: Ontological, Epistemological, Metaphysical, and Axiological Issues in Computing
Keywords: machine learning models, opaque models, practice-centered science, scientific understanding
Understanding with machines: situated practices of machine learning
This paper analyzes Emily Sullivan's article Understanding from Machine Learning Models
(2022), which argues that scientific practice using machine learning models can understand
the  phenomena  studied  without  necessarily  relying  on  transparency  or  explanatory
simplicity, maintaining that what is necessary for understanding is an empirical or scientific
link between the model and the phenomenon. In response to this proposal, I delve into
scientific understanding as understood under the correlations of these cases, and I argue
that it is not a state that is generated solely by the link that a particular scientist may boast,
but rather that the correlations that generate understanding are part of contextualized
scientific practices. Based on a critical reading of Sullivan, and with the support of works on
the philosophy of understanding (de Regt, et al., 2009; Grimm, 2014; Kelp, 2017), as well as
the philosophy of science focused on practices (Martínez and Huang, 2015; Knuuttila and
Merz, 2009; Barrera García, et al., 2023), I argue that understanding phenomena is possible
thanks to the interaction between the scientific community, scientific practices, and
machine learning models. In other words, understanding cannot be reduced to criteria of
empirical validation, but rather these must be grounded in scientific practice. I consider as
an example the research on the growth of snow crystals, from which I argue that
understanding is a contextual process, dependent on the interpretation and reconfiguration
of knowledge, in this case, through computational models.
Page 87When (if ever) is AI Non-Consequentialist?
Authors:
Ricky Mouser, Johns Hopkins Berman Institute
Topic: Ethical Issues in and of Computing
Keywords: consequentialism, machine ethics, optimization
In 2025, the explosion of artificial agents has thrust old debates in machine ethics back into
the spotlight. One central question is usually framed as follows: Should artificial agents be
consequentialist or non-consequentialist ethical reasoners?
Both sides assume we can meaningfully distinguish between consequentialist and non-
consequentialist artificial reasoners. But can we? Douglas W. Portmore argues that any non-
consequentialist moral theory can be translated into consequentialist terms. (Portmore
2009,  2022)  Say  you  think  killing  is  worse  than  letting  die.  Here’s  a  start:
the consequence  of there being a killing is worse than the consequence  of there being an
allowed death.
It’s surprisingly tricky to get clearer on where consequentialists and non-consequentialists
disagree. I begin by distinguishing between four questions. Conceptually , what does it mean
for a decision-maker to reason non-consequentially? Epistemically , what would count as
evidence that a decision-maker was reasoning non-consequentially? Epistemologically , how
well  can  we  empirically  ‘measure’  whether  a  decision-maker  is  reasoning  non-
consequentially? And metaphysically , is it even possible for an artificial decision-maker
trained via optimization to reason non-consequentially?
I argue that these questions, often conflated, should be tackled together. In doing so, the
difficulty of distinguishing consequentialist from non-consequentialist reasoning reveals that
our real disagreement is over the nature of value itself. So, I shift terrain by asking a deeper
question about the place of optimizing in ethics. Is ethics more like chess, where an optimal
playstyle awaits our discovery, or role-playing games, where the notion of an ‘optimal
playstyle’ is itself much more restricted?
By comparing responses to these four questions, I argue that my distinction between
‘optimizers’ and ‘crafters’ better tracks the shape of the underlying dispute in machine
ethics—whether we can optimize our way past apparent incommensurabilities—and offers a
cleaner path for differentiating between two fundamentally distinct styles of machine ethics.
Page 88When Set Theory meets busy beavers
Authors:
Daniel ortiz, Mathemathics department of the faculty of science unam
Topic: Theory of Computation, Formal Languages, Logic, and Foundational Mathematics
Keywords: Set theory, Turing machines, computability, radofunction
Set Theory has been the foundations par excellence for usual mathematics, capable of
encoding Recursion Theory and computability theory, both extensive theories about what
machines can do. We could think naively that Set Theory is enough for enlightening all the
truths about what machines can do, but little did we know, as godel proved for arithmetic,
formal axiom systems have big problems, even for simple, intuitive and finite objects like
numbers or in this case, Turing machines. In these talk we will discuss and explain some
problems involving Turing machines that cannot be proven via set theory or other
sufficiently rich theories. Examples such as a 745 state Turing machine that halts iff ZFC is
inconsistent and values of incomputable busybeaver functions. In the end all our theories
are made up in finitist plane just as Hilbert wanted, by taming infinity with finite resources,
we should expect some impossibilities at least right? The aim of the talk is to check some
limitations of formal systems related with computation.
 
Page 89¿Cómo imaginar futuros distintos? Ensayos desde la
transdisciplinariedad
Authors:
Grupo de Estudios Transdisciplinarios en Tecnopolítica y Cibernética, Facultad de Filosofía y Letras,
UNAM
Topic: Gender, Politics, and Society in Computing
Keywords: crítica cultural, filosofía de la tecnología, imaginarios políticos, realismo capitalista,
semiocapitalismo, tecnopolítica, transdisciplinariedad
En agosto de 2024, la Coordinación de Investigación de la Facultad de Filosofía y Letras
impulsó espacios conformados enteramente por estudiantes, entre ellos el Grupo de
Estudios  Transdisciplinarios  de  Tecnopolítica  y  Cibernética  (GETTEC),  integrado  por
estudiantes  de  filosofía,  física,  letras  inglesas,  biología,  estudios  latinoamericanos,
antropología social y ciencias de la computación. El colectivo examina críticamente las
intersecciones entre tecnología, relaciones socioculturales e imaginarios políticos, con
énfasis en el contexto latinoamericano.
Esta propuesta se enmarca en dos referencias teóricas centrales. El realismo capitalista
(Fisher, 2009) describe la imposibilidad de concebir alternativas al capitalismo, mientras
que el semiocapitalismo y el capitalismo cognitivo (Berardi, 2017) enfatizan el papel de la
producción  de  signos  y  el  trabajo  cognitivo  en  la  acumulación  contemporánea,
estrechamente vinculados al uso de tecnologías digitales. Estos marcos permiten analizar
cómo los dispositivos tecnopolíticos configuran la subjetividad y la agencia, restringiendo la
imaginación política y determinando las condiciones materiales y simbólicas del presente.
A partir de estos diagnósticos, surge la pregunta: ¿qué hacer cuando toda resistencia parece
absorbida por el sistema que se busca transformar? Desde perspectivas de análisis político,
crítica cultural y estudios sobre comunidades afectivas, se discute la viabilidad de
estrategias como el “pesimismo negativo” (Fisher), que convierte la crisis en potencia, y la
“deserción” (Berardi), que desarticula las narrativas hegemónicas. Se plantea que un futuro
postcapitalista  podría  emerger  sólo  atravesando  el  colapso  del  sistema,  mediante
resistencias  comunitarias,  afectivas  y  tecnológicas  de  carácter  público.
La mesa propone, así, imaginar y practicar futuros alternativos mediante la reapropiación
de tecnologías, la recuperación de espacios libres y la construcción de imaginarios
contrahegemónicos. Más que ofrecer soluciones definitivas, busca abrir un debate sobre las
condiciones culturales y cognitivas necesarias para interrumpir y reorientar las trayectorias
impuestas por el tecnocapitalismo contemporáneo.
Page 90Ética del Reconocimiento de las Inteligencias Artificiales en
la Producción Científica: ¿Asistentes Sintácticos o Coautores?
Authors:
HERRERA MENDEZ CARLOS MANUEL, UNIVERSIDAD DE GUADALAJARA
Topic: Computational Philosophy
Keywords: Autoría científica, Curiosidad científica, Edición de textos, Epistemología, Herramientas
algorítmicas, Inteligencias artificiales, Marco ético., Pensamiento humano, Redacción académica, Ética
académica
En esta ponencia se explora la dimensión ética del uso de inteligencias artificiales (IAs) en
la redacción académica, comparándolas con el papel tradicional de los editores. Se
argumenta que, si bien las IAs pueden optimizar la expresión sintáctica y estilística de los
textos, no deben ser reconocidas como autoras ni fuentes primarias de la investigación. Las
ideas, hipótesis y conflictos epistémicos surgen de la experiencia humana, del debate
interno y del deseo de comprender el mundo, dimensiones que escapan a la mecánica de la
IA. El trabajo propone un marco ético de reconocimiento que respete el valor humano del
pensamiento científico sin desconocer el aporte técnico de las herramientas algorítmicas.
 
Page 91“Porque tenemos aguante”: Una aproximación a la práctica
del live coding musical en América Latina
Authors:
Carlos Lingan, Facultad de Filosofía y Letras, UNAM / Facultad de Ciencias, UNAM
Topic: Aesthetic Issues in and of Computing
Keywords: Latin America, Tidal Cycles, live coding, music
El surgimiento y consolidación del live coding audiovisual y el desarrollo de entornos de
programación orientados a estas prácticas artísticas (SonicPi, Tidal Cycles o Hydra), han
dado lugar a la conformación de gran número de producciones musicales y tentativas de
reflexión en torno a las posibilidades estéticas derivadas de estos quehaceres. De esta
manera,  esta orientación artística ha permitido el encuentro entre tradiciones artísticas de
naturaleza diversa, entre las que se encuentran la música de concierto, la música
experimental y la música pop, y ha contribuido a reconocer las afinidades y las divergencias
existentes entre las estrategias compositivas y las búsquedas artísticas entre estos diversos
ámbitos sonoros [McLean y Dean, 2018; Blackwell, Cocker, Cox, McLean y Magnusson
2022]. Gracias a la emergencia de plataformas como GitHub, Soundcloud y Bandcamp, se
ha integrado un importante acervo de piezas musicales codificadas y grabadas, a través de
las que es posible perfilar estrategias de análisis de corte interdisciplinario que permitan
comprender las posibilidades artísticas y las consecuencias estéticas de esta orientación
artística. Esta ponencia ofrecerá una propuesta de análisis básica para obras sonoras
compuestas a partir de la práctica del live coding  audiovisual. Para ello, dirigirá su atención
a la producción musical de le compositore y artista visual argentine GEIKHA, y la labor de
documentación de su quehacer computacional y sonoro en los repositorios GitHub y
Soundcloud. Así, buscará analizar y caracterizar la solidaridad existente entre la concepción
de la obra musical, su implementación y realización computacional en el entorno de
programación TidalCycles, y los resultados sonoros obtenidos. A su vez, estas observaciones
permitirán identificar las afinidades y las aportaciones de le artiste argentine a los
procedimientos compositivos y las estrategias artísticas vinculadas a la estética de la
posproducción contemporánea y sus posibilidades críticas [Szendy 2003; Plasketes 2010;
Bourriaud 2021].
